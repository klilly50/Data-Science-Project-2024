{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import warnings\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Creator</th>\n",
       "      <th>UsageClass</th>\n",
       "      <th>MaterialType</th>\n",
       "      <th>Genre</th>\n",
       "      <th>CleanedPublisher</th>\n",
       "      <th>CheckoutMonth</th>\n",
       "      <th>CheckoutYear</th>\n",
       "      <th>PreviousYearCheckoutsSum</th>\n",
       "      <th>PreviousYearCheckoutsMean</th>\n",
       "      <th>PreviousYearPublished</th>\n",
       "      <th>PreviousYearPublishedMean</th>\n",
       "      <th>FirstYearCheckouts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American seafood : heritage, culture &amp; cookery...</td>\n",
       "      <td>Seaver, Barton</td>\n",
       "      <td>Physical</td>\n",
       "      <td>BOOK</td>\n",
       "      <td>other</td>\n",
       "      <td>other publisher</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>113</td>\n",
       "      <td>2.670833</td>\n",
       "      <td>1</td>\n",
       "      <td>62.046138</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The runaway pea / Kjartan Poskitt &amp; Alex Willm...</td>\n",
       "      <td>Poskitt, Kjartan</td>\n",
       "      <td>Physical</td>\n",
       "      <td>BOOK</td>\n",
       "      <td>juvenile</td>\n",
       "      <td>simon &amp; schuster</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>36</td>\n",
       "      <td>1.270833</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What the cat dragged in / Miranda James.</td>\n",
       "      <td>James, Miranda</td>\n",
       "      <td>Physical</td>\n",
       "      <td>BOOK</td>\n",
       "      <td>mystery</td>\n",
       "      <td>other publisher</td>\n",
       "      <td>9</td>\n",
       "      <td>2021</td>\n",
       "      <td>487</td>\n",
       "      <td>2.027593</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R.F.K. : a photographer's journal / Harry Benson.</td>\n",
       "      <td>Benson, Harry</td>\n",
       "      <td>Physical</td>\n",
       "      <td>BOOK</td>\n",
       "      <td>history</td>\n",
       "      <td>other publisher</td>\n",
       "      <td>11</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>63.876983</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ice Drift</td>\n",
       "      <td>Theodore Taylor</td>\n",
       "      <td>Digital</td>\n",
       "      <td>EBOOK</td>\n",
       "      <td>juvenile</td>\n",
       "      <td>harpercollins</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>36</td>\n",
       "      <td>1.322222</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title           Creator  \\\n",
       "0  American seafood : heritage, culture & cookery...    Seaver, Barton   \n",
       "1  The runaway pea / Kjartan Poskitt & Alex Willm...  Poskitt, Kjartan   \n",
       "2           What the cat dragged in / Miranda James.    James, Miranda   \n",
       "3  R.F.K. : a photographer's journal / Harry Benson.     Benson, Harry   \n",
       "4                                          Ice Drift   Theodore Taylor   \n",
       "\n",
       "  UsageClass MaterialType     Genre  CleanedPublisher  CheckoutMonth  \\\n",
       "0   Physical         BOOK     other   other publisher              1   \n",
       "1   Physical         BOOK  juvenile  simon & schuster              2   \n",
       "2   Physical         BOOK   mystery   other publisher              9   \n",
       "3   Physical         BOOK   history   other publisher             11   \n",
       "4    Digital        EBOOK  juvenile     harpercollins             11   \n",
       "\n",
       "   CheckoutYear  PreviousYearCheckoutsSum  PreviousYearCheckoutsMean  \\\n",
       "0          2018                       113                   2.670833   \n",
       "1          2022                        36                   1.270833   \n",
       "2          2021                       487                   2.027593   \n",
       "3          2008                         0                   0.000000   \n",
       "4          2013                        36                   1.322222   \n",
       "\n",
       "   PreviousYearPublished  PreviousYearPublishedMean  FirstYearCheckouts  \n",
       "0                      1                  62.046138                   9  \n",
       "1                      2                   0.000000                 167  \n",
       "2                      1                   0.000000                  31  \n",
       "3                      1                  63.876983                  12  \n",
       "4                      2                   0.000000                   4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in cleaned data\n",
    "#df_train = pd.read_csv('data/cleaned_train_2005.csv')\n",
    "df_train = pd.read_csv('data/cleaned_train_full_2005.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: \n",
    "\n",
    "For our baseline model, we predict the number of first year checkouts for a library item as the average of all the first year checkout data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make KFold object to be used on training dataset\n",
    "kfold = KFold(n_splits = 5,\n",
    "              shuffle = True,\n",
    "              random_state = 216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses = np.zeros(5)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(df_train, df_train['FirstYearCheckouts'])):\n",
    "    ## get the kfold training and holdout data\n",
    "    X_tt = df_train.iloc[train_index]\n",
    "    X_ho = df_train.iloc[test_index]\n",
    "\n",
    "    #Get average of the first year checkouts for our train set \n",
    "    baseline_pred = np.ones(len(X_ho)) * X_tt['FirstYearCheckouts'].mean() \n",
    "\n",
    "    ## Record the rmses\n",
    "    rmses[i] = root_mean_squared_error(X_ho['FirstYearCheckouts'], baseline_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[114.72254652 121.97266529 109.23361391 118.73402191 115.23831321]\n",
      "115.98023216785975\n"
     ]
    }
   ],
   "source": [
    "print(rmses)\n",
    "print(rmses.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of first year checkouts for the whole training dataset is 44.87509717142807\n",
      "The median number of first year checkouts in our dataset is 21.0\n",
      "The minimum and maximum first year checkouts in our dataset are 1 and 9633 respectively\n"
     ]
    }
   ],
   "source": [
    "print('The average number of first year checkouts for the whole training dataset is', df_train['FirstYearCheckouts'].mean())\n",
    "print('The median number of first year checkouts in our dataset is', np.median(df_train['FirstYearCheckouts']))\n",
    "print('The minimum and maximum first year checkouts in our dataset are', df_train['FirstYearCheckouts'].min(), \\\n",
    "      'and', df_train['FirstYearCheckouts'].max(), 'respectively')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our median and average are pretty low compared with the maximum we see; our distribution seems to have a long tail, so we will likely have higher RSMEs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Creator', 'UsageClass', 'MaterialType', 'Genre',\n",
       "       'CleanedPublisher', 'CheckoutMonth', 'CheckoutYear',\n",
       "       'PreviousYearCheckoutsSum', 'PreviousYearCheckoutsMean',\n",
       "       'PreviousYearPublished', 'PreviousYearPublishedMean',\n",
       "       'FirstYearCheckouts'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Modeling Attempt: Linear Regression\n",
    "\n",
    "For our first model, we will consider linear regression. We will train on all features outside of Title and Creator, and we one-hot encode the categorical variables UsageClass, MaterialType, Genre, CleanedPublisher. Since UsageClass is binary, we will replace this column with a 1 if the item is Physical and 0 if the item is Digital. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of cateories for columns of one-hot encoding\n",
    "genre_list = df_train['Genre'].unique().tolist()\n",
    "material_list = df_train['MaterialType'].unique().tolist()\n",
    "publisher_list = df_train['CleanedPublisher'].unique().tolist()\n",
    "\n",
    "# One-hot encoding of 'Genre' variable\n",
    "df_train[genre_list] = pd.get_dummies(df_train['Genre'])\n",
    "\n",
    "# One-hot encoding of 'MaterialType' variables\n",
    "df_train[material_list] = pd.get_dummies(df_train['MaterialType'])\n",
    "\n",
    "# One-hot encoding of 'CleanedPublisher' variable\n",
    "df_train[publisher_list] = pd.get_dummies(df_train['CleanedPublisher'])\n",
    "\n",
    "#One hot encode UsageClass into single column with 1 indicating 'Physical' and 0 indicating 'Digital'\n",
    "df_train['UsageClass'] = pd.get_dummies(df_train['UsageClass'])['Physical']\n",
    "#df_train = df_train.drop(columns = ['Genre', 'MaterialType'])\n",
    "df_train = df_train.drop(columns = ['Genre', 'MaterialType', 'CleanedPublisher'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Creator', 'UsageClass', 'CheckoutMonth', 'CheckoutYear',\n",
       "       'PreviousYearCheckoutsSum', 'PreviousYearCheckoutsMean',\n",
       "       'PreviousYearPublished', 'PreviousYearPublishedMean',\n",
       "       'FirstYearCheckouts', 'other', 'juvenile', 'mystery', 'history',\n",
       "       'nonfiction', 'fiction', 'romance', 'fantasy/sci-fi', 'biography',\n",
       "       'young adult', 'horror/thriller', 'BOOK', 'EBOOK', 'AUDIOBOOK',\n",
       "       'SOUNDDISC', 'OTHER', 'VIDEODISC', 'other publisher',\n",
       "       'simon & schuster', 'harpercollins', 'penguin random house', 'hachette',\n",
       "       'blackstone', 'harlequin', 'macmillan', 'scholastic',\n",
       "       'lightning source', 'recorded books', 'brilliance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features we want to train our linear regression on\n",
    "features =['UsageClass', 'CheckoutMonth', 'CheckoutYear',\n",
    "       'PreviousYearCheckoutsSum', 'PreviousYearCheckoutsMean',\n",
    "       'PreviousYearPublished', 'PreviousYearPublishedMean'\n",
    "       , 'other', 'juvenile', 'mystery', 'history',\n",
    "       'nonfiction', 'fiction', 'romance', 'fantasy/sci-fi', 'biography',\n",
    "       'young adult', 'horror/thriller', 'BOOK', 'EBOOK', 'AUDIOBOOK',\n",
    "       'SOUNDDISC', 'OTHER', 'VIDEODISC', 'other publisher',\n",
    "       'simon & schuster', 'harpercollins', 'penguin random house', 'hachette',\n",
    "       'blackstone', 'harlequin', 'macmillan', 'scholastic',\n",
    "       'lightning source', 'recorded books', 'brilliance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kfold split\n",
    "kfold = KFold(n_splits = 5,\n",
    "              shuffle = True,\n",
    "              random_state = 216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to store rmse\n",
    "rmse = np.zeros(5)\n",
    "\n",
    "# Initialize LinearRegression Model\n",
    "lr = LinearRegression()\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(df_train, df_train['FirstYearCheckouts'])):\n",
    "\n",
    "    ## get the kfold training and holdout data\n",
    "    X_tt = df_train.iloc[train_index]\n",
    "    X_ho = df_train.iloc[test_index]\n",
    "\n",
    "    ## Fit model\n",
    "    lr.fit(X_tt[features], X_tt['FirstYearCheckouts'])\n",
    "\n",
    "    ## Generate predictions on the holdout set\n",
    "    lr_preds = lr.predict(X_ho[features])\n",
    "\n",
    "    ## Record the rmses\n",
    "    rmses[i] = root_mean_squared_error(X_ho['FirstYearCheckouts'], lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109.98766426 117.18175179 104.24685806 114.22175667 109.89098079]\n",
      "111.10580231523207\n"
     ]
    }
   ],
   "source": [
    "print(rmses)\n",
    "print(rmses.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Regularization: \n",
    "\n",
    "We add lasso regularization to our model to see what features are most important. We first run the regularizaiton with alpha = 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kfold split\n",
    "kfold = KFold(n_splits = 5,\n",
    "              shuffle = True,\n",
    "              random_state = 216)\n",
    "\n",
    "# Array to store rmse\n",
    "rmse = np.zeros(5)\n",
    "\n",
    "#Initialize Lasso regression model\n",
    "lasso = Lasso(alpha=0.1)  \n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(df_train, df_train['FirstYearCheckouts'])):\n",
    "\n",
    "    ## get the kfold training and holdout data\n",
    "    X_tt = df_train.iloc[train_index]\n",
    "    X_ho = df_train.iloc[test_index]\n",
    "\n",
    "    ## Fit model\n",
    "    lasso.fit(X_tt[features], X_tt['FirstYearCheckouts'])\n",
    "\n",
    "\n",
    "    ## Generate predictions on the holdout set\n",
    "    lasso_preds = lasso.predict(X_ho[features])\n",
    "\n",
    "    ## Record the rmses\n",
    "    rmses[i] = root_mean_squared_error(X_ho['FirstYearCheckouts'], lasso_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110.01183039 117.20027996 104.26801716 114.2452093  109.94932836]\n",
      "111.13493303349594\n"
     ]
    }
   ],
   "source": [
    "print(rmses)\n",
    "print(rmses.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this behaves very similarly to linear regression without regularization. We now search through different alpha values to see which values gives the best model and what features are most important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.369e+08, tolerance: 6.099e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.369e+08, tolerance: 6.099e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.369e+08, tolerance: 6.099e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "\n",
      "0.1\n",
      "\n",
      "1\n",
      "\n",
      "10\n",
      "\n",
      "100\n",
      "\n",
      "1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "alpha = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "n =len(features)\n",
    "\n",
    "lasso_coefs = np.empty((len(alpha),n))\n",
    "rmses = np.empty(len(alpha))\n",
    "\n",
    "## for each alpha value\n",
    "for i in range(len(alpha)):\n",
    "    print(alpha[i])\n",
    "    print()\n",
    "    \n",
    "    ## set up the lasso pipeline\n",
    "    lasso_pipe = Pipeline([('scale',StandardScaler()),\n",
    "                              ('lasso', Lasso(alpha=alpha[i]))\n",
    "                          ])\n",
    "    \n",
    "    ## fit the lasso\n",
    "    lasso_pipe.fit(df_train[features], df_train['FirstYearCheckouts'])\n",
    "\n",
    "    ## predict \n",
    "    pred = lasso_pipe.predict(df_train[features])\n",
    "    rmses[i] = root_mean_squared_error(pred, df_train['FirstYearCheckouts'])\n",
    "\n",
    "\n",
    "    # record the coefficients\n",
    "    lasso_coefs[i,:] = lasso_pipe['lasso'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Coefficients\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UsageClass</th>\n",
       "      <th>CheckoutMonth</th>\n",
       "      <th>CheckoutYear</th>\n",
       "      <th>PreviousYearCheckoutsSum</th>\n",
       "      <th>PreviousYearCheckoutsMean</th>\n",
       "      <th>PreviousYearPublished</th>\n",
       "      <th>PreviousYearPublishedMean</th>\n",
       "      <th>other</th>\n",
       "      <th>juvenile</th>\n",
       "      <th>mystery</th>\n",
       "      <th>history</th>\n",
       "      <th>nonfiction</th>\n",
       "      <th>fiction</th>\n",
       "      <th>romance</th>\n",
       "      <th>fantasy/sci-fi</th>\n",
       "      <th>biography</th>\n",
       "      <th>young adult</th>\n",
       "      <th>horror/thriller</th>\n",
       "      <th>BOOK</th>\n",
       "      <th>EBOOK</th>\n",
       "      <th>AUDIOBOOK</th>\n",
       "      <th>SOUNDDISC</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>VIDEODISC</th>\n",
       "      <th>other publisher</th>\n",
       "      <th>simon &amp; schuster</th>\n",
       "      <th>harpercollins</th>\n",
       "      <th>penguin random house</th>\n",
       "      <th>hachette</th>\n",
       "      <th>blackstone</th>\n",
       "      <th>harlequin</th>\n",
       "      <th>macmillan</th>\n",
       "      <th>scholastic</th>\n",
       "      <th>lightning source</th>\n",
       "      <th>recorded books</th>\n",
       "      <th>brilliance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha=1e-05</th>\n",
       "      <td>1.664975</td>\n",
       "      <td>-0.964015</td>\n",
       "      <td>2.154109</td>\n",
       "      <td>10.139801</td>\n",
       "      <td>25.859853</td>\n",
       "      <td>-5.236976</td>\n",
       "      <td>-0.107529</td>\n",
       "      <td>1.812815</td>\n",
       "      <td>1.265832</td>\n",
       "      <td>3.098828</td>\n",
       "      <td>-0.885583</td>\n",
       "      <td>3.441402</td>\n",
       "      <td>-4.046213</td>\n",
       "      <td>1.388172</td>\n",
       "      <td>1.467246</td>\n",
       "      <td>-0.232567</td>\n",
       "      <td>1.666957</td>\n",
       "      <td>-2.078180</td>\n",
       "      <td>1.876726</td>\n",
       "      <td>9.303623</td>\n",
       "      <td>-4.319153</td>\n",
       "      <td>-1.559242</td>\n",
       "      <td>3.162130</td>\n",
       "      <td>0.666570</td>\n",
       "      <td>-2.300159</td>\n",
       "      <td>-3.481724</td>\n",
       "      <td>-0.521887</td>\n",
       "      <td>-0.105125</td>\n",
       "      <td>1.384022</td>\n",
       "      <td>-0.367599</td>\n",
       "      <td>0.848916</td>\n",
       "      <td>-3.964979</td>\n",
       "      <td>2.592150</td>\n",
       "      <td>-1.943754</td>\n",
       "      <td>-0.736035</td>\n",
       "      <td>2.041423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=0.0001</th>\n",
       "      <td>1.664058</td>\n",
       "      <td>-0.963929</td>\n",
       "      <td>2.154056</td>\n",
       "      <td>10.139687</td>\n",
       "      <td>25.859816</td>\n",
       "      <td>-5.236870</td>\n",
       "      <td>-0.107438</td>\n",
       "      <td>1.782882</td>\n",
       "      <td>1.237379</td>\n",
       "      <td>3.058264</td>\n",
       "      <td>-0.912019</td>\n",
       "      <td>3.417605</td>\n",
       "      <td>-4.093303</td>\n",
       "      <td>1.360306</td>\n",
       "      <td>1.433842</td>\n",
       "      <td>-0.279802</td>\n",
       "      <td>1.641160</td>\n",
       "      <td>-2.099282</td>\n",
       "      <td>1.829955</td>\n",
       "      <td>9.231508</td>\n",
       "      <td>-4.386252</td>\n",
       "      <td>-1.577137</td>\n",
       "      <td>3.123854</td>\n",
       "      <td>0.662750</td>\n",
       "      <td>-2.285572</td>\n",
       "      <td>-3.472933</td>\n",
       "      <td>-0.506179</td>\n",
       "      <td>-0.096145</td>\n",
       "      <td>1.411146</td>\n",
       "      <td>-0.356666</td>\n",
       "      <td>0.867538</td>\n",
       "      <td>-3.914986</td>\n",
       "      <td>2.627753</td>\n",
       "      <td>-1.928767</td>\n",
       "      <td>-0.723138</td>\n",
       "      <td>2.060568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=0.001</th>\n",
       "      <td>1.654891</td>\n",
       "      <td>-0.963070</td>\n",
       "      <td>2.153522</td>\n",
       "      <td>10.138550</td>\n",
       "      <td>25.859449</td>\n",
       "      <td>-5.235815</td>\n",
       "      <td>-0.106528</td>\n",
       "      <td>1.483553</td>\n",
       "      <td>0.952853</td>\n",
       "      <td>2.652626</td>\n",
       "      <td>-1.176379</td>\n",
       "      <td>3.179631</td>\n",
       "      <td>-4.564208</td>\n",
       "      <td>1.081643</td>\n",
       "      <td>1.099807</td>\n",
       "      <td>-0.752154</td>\n",
       "      <td>1.383193</td>\n",
       "      <td>-2.310301</td>\n",
       "      <td>1.362250</td>\n",
       "      <td>8.510351</td>\n",
       "      <td>-5.057241</td>\n",
       "      <td>-1.756090</td>\n",
       "      <td>2.741104</td>\n",
       "      <td>0.624550</td>\n",
       "      <td>-2.139704</td>\n",
       "      <td>-3.385022</td>\n",
       "      <td>-0.349091</td>\n",
       "      <td>-0.006343</td>\n",
       "      <td>1.682389</td>\n",
       "      <td>-0.247339</td>\n",
       "      <td>1.053758</td>\n",
       "      <td>-3.415061</td>\n",
       "      <td>2.983781</td>\n",
       "      <td>-1.778897</td>\n",
       "      <td>-0.594171</td>\n",
       "      <td>2.252015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=0.01</th>\n",
       "      <td>1.993023</td>\n",
       "      <td>-0.955239</td>\n",
       "      <td>2.141577</td>\n",
       "      <td>10.127746</td>\n",
       "      <td>25.853330</td>\n",
       "      <td>-5.222673</td>\n",
       "      <td>-0.097726</td>\n",
       "      <td>0.482386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.297713</td>\n",
       "      <td>-2.050391</td>\n",
       "      <td>2.383210</td>\n",
       "      <td>-6.125209</td>\n",
       "      <td>0.147401</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-2.315385</td>\n",
       "      <td>0.520149</td>\n",
       "      <td>-3.007046</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>6.015137</td>\n",
       "      <td>-7.012715</td>\n",
       "      <td>-2.315806</td>\n",
       "      <td>1.413071</td>\n",
       "      <td>0.486074</td>\n",
       "      <td>-1.806879</td>\n",
       "      <td>-3.180415</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.182016</td>\n",
       "      <td>2.283397</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.463387</td>\n",
       "      <td>-2.285995</td>\n",
       "      <td>3.769654</td>\n",
       "      <td>-1.436665</td>\n",
       "      <td>-0.298573</td>\n",
       "      <td>2.672745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=0.1</th>\n",
       "      <td>4.257006</td>\n",
       "      <td>-0.875235</td>\n",
       "      <td>2.038845</td>\n",
       "      <td>10.016275</td>\n",
       "      <td>25.793761</td>\n",
       "      <td>-5.100807</td>\n",
       "      <td>-0.008933</td>\n",
       "      <td>0.447162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.265386</td>\n",
       "      <td>-1.928238</td>\n",
       "      <td>2.334171</td>\n",
       "      <td>-5.977353</td>\n",
       "      <td>0.075079</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-2.119430</td>\n",
       "      <td>0.444656</td>\n",
       "      <td>-2.917202</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>3.465494</td>\n",
       "      <td>-6.910069</td>\n",
       "      <td>-2.518260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263009</td>\n",
       "      <td>-1.727428</td>\n",
       "      <td>-3.087556</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.078069</td>\n",
       "      <td>2.159719</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.331534</td>\n",
       "      <td>-2.257082</td>\n",
       "      <td>3.585012</td>\n",
       "      <td>-1.350459</td>\n",
       "      <td>-0.233581</td>\n",
       "      <td>2.544852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=1</th>\n",
       "      <td>2.692603</td>\n",
       "      <td>-0.030059</td>\n",
       "      <td>1.496581</td>\n",
       "      <td>8.966051</td>\n",
       "      <td>25.281457</td>\n",
       "      <td>-4.032972</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.342630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.208996</td>\n",
       "      <td>-0.483923</td>\n",
       "      <td>1.992429</td>\n",
       "      <td>-4.029570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.810563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.639780</td>\n",
       "      <td>-5.010462</td>\n",
       "      <td>-1.721461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.628492</td>\n",
       "      <td>-2.073609</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.123811</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.150641</td>\n",
       "      <td>-1.865294</td>\n",
       "      <td>2.097519</td>\n",
       "      <td>-0.233642</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623988</td>\n",
       "      <td>18.233488</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=100</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=1000</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              UsageClass  CheckoutMonth  CheckoutYear  \\\n",
       "alpha=1e-05     1.664975      -0.964015      2.154109   \n",
       "alpha=0.0001    1.664058      -0.963929      2.154056   \n",
       "alpha=0.001     1.654891      -0.963070      2.153522   \n",
       "alpha=0.01      1.993023      -0.955239      2.141577   \n",
       "alpha=0.1       4.257006      -0.875235      2.038845   \n",
       "alpha=1         2.692603      -0.030059      1.496581   \n",
       "alpha=10        0.000000      -0.000000      0.000000   \n",
       "alpha=100       0.000000      -0.000000      0.000000   \n",
       "alpha=1000      0.000000      -0.000000      0.000000   \n",
       "\n",
       "              PreviousYearCheckoutsSum  PreviousYearCheckoutsMean  \\\n",
       "alpha=1e-05                  10.139801                  25.859853   \n",
       "alpha=0.0001                 10.139687                  25.859816   \n",
       "alpha=0.001                  10.138550                  25.859449   \n",
       "alpha=0.01                   10.127746                  25.853330   \n",
       "alpha=0.1                    10.016275                  25.793761   \n",
       "alpha=1                       8.966051                  25.281457   \n",
       "alpha=10                      0.623988                  18.233488   \n",
       "alpha=100                     0.000000                   0.000000   \n",
       "alpha=1000                    0.000000                   0.000000   \n",
       "\n",
       "              PreviousYearPublished  PreviousYearPublishedMean     other  \\\n",
       "alpha=1e-05               -5.236976                  -0.107529  1.812815   \n",
       "alpha=0.0001              -5.236870                  -0.107438  1.782882   \n",
       "alpha=0.001               -5.235815                  -0.106528  1.483553   \n",
       "alpha=0.01                -5.222673                  -0.097726  0.482386   \n",
       "alpha=0.1                 -5.100807                  -0.008933  0.447162   \n",
       "alpha=1                   -4.032972                  -0.000000  0.342630   \n",
       "alpha=10                  -0.000000                  -0.000000  0.000000   \n",
       "alpha=100                 -0.000000                  -0.000000  0.000000   \n",
       "alpha=1000                -0.000000                  -0.000000  0.000000   \n",
       "\n",
       "              juvenile   mystery   history  nonfiction   fiction   romance  \\\n",
       "alpha=1e-05   1.265832  3.098828 -0.885583    3.441402 -4.046213  1.388172   \n",
       "alpha=0.0001  1.237379  3.058264 -0.912019    3.417605 -4.093303  1.360306   \n",
       "alpha=0.001   0.952853  2.652626 -1.176379    3.179631 -4.564208  1.081643   \n",
       "alpha=0.01    0.000000  1.297713 -2.050391    2.383210 -6.125209  0.147401   \n",
       "alpha=0.1     0.000000  1.265386 -1.928238    2.334171 -5.977353  0.075079   \n",
       "alpha=1       0.000000  1.208996 -0.483923    1.992429 -4.029570  0.000000   \n",
       "alpha=10      0.000000  0.000000 -0.000000    0.000000 -0.000000  0.000000   \n",
       "alpha=100     0.000000  0.000000 -0.000000    0.000000 -0.000000  0.000000   \n",
       "alpha=1000    0.000000  0.000000 -0.000000    0.000000 -0.000000  0.000000   \n",
       "\n",
       "              fantasy/sci-fi  biography  young adult  horror/thriller  \\\n",
       "alpha=1e-05         1.467246  -0.232567     1.666957        -2.078180   \n",
       "alpha=0.0001        1.433842  -0.279802     1.641160        -2.099282   \n",
       "alpha=0.001         1.099807  -0.752154     1.383193        -2.310301   \n",
       "alpha=0.01         -0.000681  -2.315385     0.520149        -3.007046   \n",
       "alpha=0.1          -0.000000  -2.119430     0.444656        -2.917202   \n",
       "alpha=1            -0.000000  -0.000000     0.000000        -1.810563   \n",
       "alpha=10           -0.000000   0.000000    -0.000000        -0.000000   \n",
       "alpha=100          -0.000000  -0.000000     0.000000        -0.000000   \n",
       "alpha=1000         -0.000000  -0.000000     0.000000        -0.000000   \n",
       "\n",
       "                  BOOK     EBOOK  AUDIOBOOK  SOUNDDISC     OTHER  VIDEODISC  \\\n",
       "alpha=1e-05   1.876726  9.303623  -4.319153  -1.559242  3.162130   0.666570   \n",
       "alpha=0.0001  1.829955  9.231508  -4.386252  -1.577137  3.123854   0.662750   \n",
       "alpha=0.001   1.362250  8.510351  -5.057241  -1.756090  2.741104   0.624550   \n",
       "alpha=0.01   -0.000000  6.015137  -7.012715  -2.315806  1.413071   0.486074   \n",
       "alpha=0.1    -0.000000  3.465494  -6.910069  -2.518260  0.000000   0.263009   \n",
       "alpha=1       0.000000  3.639780  -5.010462  -1.721461  0.000000   0.000000   \n",
       "alpha=10      0.000000  0.000000  -0.000000  -0.000000  0.000000   0.000000   \n",
       "alpha=100     0.000000  0.000000  -0.000000  -0.000000  0.000000   0.000000   \n",
       "alpha=1000    0.000000  0.000000  -0.000000  -0.000000  0.000000   0.000000   \n",
       "\n",
       "              other publisher  simon & schuster  harpercollins  \\\n",
       "alpha=1e-05         -2.300159         -3.481724      -0.521887   \n",
       "alpha=0.0001        -2.285572         -3.472933      -0.506179   \n",
       "alpha=0.001         -2.139704         -3.385022      -0.349091   \n",
       "alpha=0.01          -1.806879         -3.180415      -0.000000   \n",
       "alpha=0.1           -1.727428         -3.087556      -0.000000   \n",
       "alpha=1             -0.628492         -2.073609      -0.000000   \n",
       "alpha=10            -0.000000         -0.000000      -0.000000   \n",
       "alpha=100           -0.000000         -0.000000      -0.000000   \n",
       "alpha=1000          -0.000000         -0.000000      -0.000000   \n",
       "\n",
       "              penguin random house  hachette  blackstone  harlequin  \\\n",
       "alpha=1e-05              -0.105125  1.384022   -0.367599   0.848916   \n",
       "alpha=0.0001             -0.096145  1.411146   -0.356666   0.867538   \n",
       "alpha=0.001              -0.006343  1.682389   -0.247339   1.053758   \n",
       "alpha=0.01                0.182016  2.283397   -0.000000   1.463387   \n",
       "alpha=0.1                 0.078069  2.159719   -0.000000   1.331534   \n",
       "alpha=1                   0.000000  1.123811   -0.000000   0.150641   \n",
       "alpha=10                 -0.000000  0.000000   -0.000000   0.000000   \n",
       "alpha=100                -0.000000  0.000000   -0.000000   0.000000   \n",
       "alpha=1000               -0.000000  0.000000   -0.000000   0.000000   \n",
       "\n",
       "              macmillan  scholastic  lightning source  recorded books  \\\n",
       "alpha=1e-05   -3.964979    2.592150         -1.943754       -0.736035   \n",
       "alpha=0.0001  -3.914986    2.627753         -1.928767       -0.723138   \n",
       "alpha=0.001   -3.415061    2.983781         -1.778897       -0.594171   \n",
       "alpha=0.01    -2.285995    3.769654         -1.436665       -0.298573   \n",
       "alpha=0.1     -2.257082    3.585012         -1.350459       -0.233581   \n",
       "alpha=1       -1.865294    2.097519         -0.233642       -0.000000   \n",
       "alpha=10      -0.000000    0.000000         -0.000000        0.000000   \n",
       "alpha=100     -0.000000    0.000000         -0.000000        0.000000   \n",
       "alpha=1000    -0.000000    0.000000         -0.000000        0.000000   \n",
       "\n",
       "              brilliance  \n",
       "alpha=1e-05     2.041423  \n",
       "alpha=0.0001    2.060568  \n",
       "alpha=0.001     2.252015  \n",
       "alpha=0.01      2.672745  \n",
       "alpha=0.1       2.544852  \n",
       "alpha=1         1.376000  \n",
       "alpha=10        0.000000  \n",
       "alpha=100       0.000000  \n",
       "alpha=1000      0.000000  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Lasso Coefficients\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.DataFrame(np.round(lasso_coefs,8),\n",
    "            columns = features,\n",
    "            index = [\"alpha=\" + str(a) for a in alpha])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PreviousYearCheckoutsMean is the feature with the coefficient of highest magnitude, with PreviousYearCheckoutSum and PreviousYearPublished also of larger magnitude. It is interesting to note that PreviousYearPublished has negative coefficients so that authors with books written in the last year have fewer checkouts.   UsageClass and CheckoutYear have coefficients of medium magntiude. In the Genre category, most of the coefficients are smaller, though those of nonfiction and fiction are of medium magnitude. Of the MaterialType categories, EBOOK and AUDIOBOOK are the most important with a positive coefficient for EBOOK but a negative one for AUDIOBOOKS. There are a few publishers with coefficients that are of slightly larger magnitude compared with the others, though most of the coefficients still stay on the smaller side.  We note that for large alpha values, none of the features are good indicators of FirstYearCheckout as all of the features zero out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE by alpha: \n",
      "Alpha 1e-05 : 111.17017748421029\n",
      "Alpha 0.0001 : 111.1701774859389\n",
      "Alpha 0.001 : 111.17017765880108\n",
      "Alpha 0.01 : 111.17020068991974\n",
      "Alpha 0.1 : 111.1721580317754\n",
      "Alpha 1 : 111.29169576805603\n",
      "Alpha 10 : 112.92470439838675\n",
      "Alpha 100 : 116.05858206189905\n",
      "Alpha 1000 : 116.05858206189905\n"
     ]
    }
   ],
   "source": [
    "print('RMSE by alpha: ')\n",
    "for i in range(len(alpha)):\n",
    "    print('Alpha', alpha[i], ':', rmses[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all of the alpha values give similar RMSEs compared with normal linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Models: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try a Random Forest, which should allow for non-linearity and will hopefully perform better as we do not expect our data to behave linearly based on earlier data exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 103.95280888503329\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df_tt, df_ho = train_test_split(df_train, test_size=0.2,\\\n",
    "                                      shuffle=True, random_state=216)\n",
    "\n",
    "# Create a Random Forest Regressor model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(df_tt[features], df_tt['FirstYearCheckouts'])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(df_ho[features])\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = root_mean_squared_error(df_ho['FirstYearCheckouts'], y_pred)\n",
    "print(\"Root Mean Squared Error:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This improves are error by about 10 checkouts, which is not a huge improvement, but still noticable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use KNN Regressor to model our problem and compare it with the RandomForest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 109.43480298078369\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_tt, df_ho = train_test_split(df_train, test_size=0.2,\\\n",
    "                                      shuffle=True, random_state=216)\n",
    "\n",
    "neigh = Pipeline([ ( 'scale', StandardScaler()), ('knn', KNeighborsRegressor())])\n",
    "\n",
    "neigh.fit(df_tt[features], df_tt['FirstYearCheckouts'])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = neigh.predict(df_ho[features])\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = root_mean_squared_error(df_ho['FirstYearCheckouts'], y_pred)\n",
    "print(\"Root Mean Squared Error:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still a slight improvement over the baseline model, but does worse than the RandomForest model. We now do a comparison of the RMSE from different numbers of neighbors to see what performs best; we only run this on a smaller subset of features for computational convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjYAAAGsCAYAAADOo+2NAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABjMklEQVR4nO3dd3wUdf7H8deW9ARSSAi9BxQDBKRJRwFFYkPEE7HggfjjKBFRTjn1UJTDgoKinoDoiaIIKHIiqKhHkQhIiSK9CUhLSEgvu/v7Y5OFJUEDJJnN5v18PPLI7HdmZz6Zb5a8mfnOjMnhcDgQERER8QJmowsQERERKSsKNiIiIuI1FGxERETEayjYiIiIiNdQsBERERGvoWAjIiIiXkPBRkRERLyG1egCKprdbqegoACz2YzJZDK6HBERESkFh8OB3W7HarViNl/4uEyVCzYFBQUkJSUZXYaIiIhcgtjYWHx9fS84v8oFm6KUFxsbi8ViMbgaz2Oz2UhKStL+8RDqD8+jPvEs6g/PUp79UbTuPzpaA1Uw2BSdfrJYLPoQ/AHtH8+i/vA86hPPov7wLOXZH382jESDh0VERMRrKNiIiIiI11CwEREREa9R5cbYiIh4OpvNRn5+vtFlVAo2mw2AnJwcjbHxAJfTHz4+PmXShwo2IiIewuFwcOzYMVJTU40updJwOBxYrVYOHjyoe5N5gMvtj9DQUKKjoy+rLxVsREQ8RFGoiYqKIjAwUH+oS8HhcJCdnU1AQID2lwe41P5wOBxkZWVx4sQJAGrVqnXJNSjYiIh4AJvN5go1ERERRpdTaRTdjdbf31/BxgNcTn8EBAQAcOLECaKioi75tJQGD4uIeICiMTWBgYEGVyJinKLf/8sZY6ZgIyLiQXTUQaqysvj9V7ARERERr6FgIyIiHunAgQNGl1AqFVXniRMnyMrKqpBtVWaGB5uUlBT69OlDYmKiW/vmzZuJjY0ttvyKFSsYMGAAbdq0oU+fPnzyyScVVaqIiJznySefJC4ujri4OGJjY2nRooXrdVxcHBs3bryk9W7fvp0BAwa4befJJ58sq7L/0MyZMxk6dGipll21ahUPPPBAOVcEp06dol+/fqSkpJQ4f/HixcX2fevWrenVqxcvvfQSDofDtVzz5s0ZPHhwieu56aabaN68OYcPHwYgLy+Pl156ieuuu464uDg6derE6NGj2bt3r+s9M2fO5IorriAuLo62bdvSpUsX2rZtS1xcXIX12bkMvSpq06ZNTJw4kUOHDrnaHA4HixYtYsqUKeTl5bktv379eiZOnMgrr7xC9+7dSUxMZPjw4cTExNCqVauKLt+N3e4gz2bH30c3iBKRsuNwOMjOt1XoNgN8LKUe6zB58mQmT54MOP9ovvbaa6xateqya0hPT3cbQFq0DU+TmprqCg3lKScn50+P1tSuXdtt39tsNr7//nvGjBlD/fr1GTRoEAAhISH88ssv7Nu3j8aNG7uWT0pK4siRI27rfOaZZ9i/fz/z5s2jbt26nDlzhpkzZzJkyBBWrlxJtWrVALj66qv5z3/+47ps28jbFRgWbJYsWcKMGTOYMGECCQkJrvbHH3+cffv2MWbMGKZOner2nnnz5nHPPffQo0cPADp16sSiRYuIioq66O0X3R2xrAz+dyKHT2ex8MFO1A4NKNN1V6Si/VLW+0cujfrD85RXn9hsNhwOh+sLnKFm0Fvr2XTwdJlu689c3SCMjx/sdNF/mM6vH+DQoUM899xzbNmyhcDAQOLj4xk1ahS+vr5kZGTw5JNPsm7dOqxWK82bN+fxxx/H19eX4cOHAxAXF8ecOXP46KOPAJg6dSozZ85k9+7d+Pn58d133xEQEMDNN9/M+PHjAWcImDp1KsuXLycgIIBbb72VpUuX8txzz9GxY8didf/0008899xz7N27lxYtWlC/fn23n2X27NksXbqUY8eOYTKZ6N69O88++yxbt27lqaeeIj8/n7i4OJYvX+6qcdu2bSQnJ1OjRg0eeughBg4cCMCHH37I3LlzSU1NpVatWgwdOtQVOC60rywWi+vo1YABA5gyZQr9+/f/031vNpvp1asXMTExbN++3TU/JCSEDh06sGTJEh5++GHX8osWLaJ///58/PHHrmU3bdrETTfdRJ06dVzvnTBhAhkZGZw8eZKQkBC3bZ/7u3spitZhs9mKfcZK+5kzLNh07dqV+Ph4rFarW7AZO3Ys0dHRxU5NAWzbto2OHTsyYsQItm7dSnR0NKNHjyYmJuait5+UlHRZ9Z8vLzuTY2fyGDlvHU/1CMdSya9sKOv9I5dH/eF5yqNPrFYr2dnZ2O12oOieIBUfam12G1lZWRcdbPLy8lz/YwfIzs7m3nvv5frrr+f555/n9OnTPProo+Tl5TF69Gjeeust0tLS+OKLLzCbzUyZMoVp06Yxffp0Zs6cyYgRI1izZo2zpsI/allZWeTn5/PVV1/xz3/+kyeffJIffviBsWPH0qVLF1q1asWUKVPYvn07H374IdWqVeP555/nyJEj5ObmFjvqkZqayoMPPsj999/PnDlz+OWXXxgzZgzNmzcnKyuLlStX8u677zJ79mzq16/P/v37GTZsGIsXL+aWW27h8ccf56233uK///0vAKNHj6Z69ep8/PHH+Pr68uGHH/LMM8/Qo0cPUlJSeP7551mwYAENGzZk3bp1jB8/ng4dOhAcHPyH+2rhwoUMGDCAhQsXUrt27WI/x/n7HpyXTH/33Xfs3LmTBx98kKysLNdy/fv3Z9q0aYwYMQKz2Uxubi7Lly9n+vTpfPzxx64jRH379uX1119nz549tG/fntjYWBo0aMCkSZPc+sNut7ttOzs7+6J+d4rk5uaSn5/Pjh07Lun9YGCwiYyMLLE9Ojr6gu9JS0tjzpw5zJw5k9jYWFatWkVCQgLvv/8+rVu3vqjtx8bGlulzRabXyyT+tXX8cjKfjenVebB74z9/kwey2WwkJSWV+f6RS6P+8Dzl1Sc5OTkcPHiQgIAA/P39Xe2LHuri0aeizuXr64vJZHLdi+T777+noKCARx99FJPJRFhYGOPGjWPs2LE89thjhISEsHv3blauXEnXrl3517/+hdnsHPrp5+cHnL2vSdG+DgwMxMfHh4YNG3LHHXfgcDjo2rUrkZGRHDt2jHbt2vHFF18wY8YM12mWf/7zn3z55Zf4+fkVu0/QihUrCAwM5KGHHsJkMnHNNdcwcOBAtm/fTmBgINdddx2dOnUiOjqalJQUcnJyCAsL4/Tp0wQGBhb7mZ977jmCgoLw9/fn2LFjhIaGkpOTQ15eHsHBwTgcDj777DP69etHz5492bx5M2azmeXLl//hvir6nfD39y/xXke+vr4cO3aMHj164HA4XAHmmmuuYcaMGfTu3dutj/r27cuUKVPYsmULXbt25ZtvvqF169bUq1fPbTtjx44lNjaWzz77jFdeeYWUlBSioqIYNmwY9913H+B8xtOWLVtcZ1McDofr9+fNN9+kXbt2pf4dMpvN+Pj40LRpU7fPAZz97P2ZSnXnYV9fXwYOHEhcXBwAffv2pXPnzqxYseKig43FYinTf5SaRFXj6fiWPLpoG9O/3k33mCiuqlO9zNZf0cp6/8jlUX94nrLuE4vFGSaKvoqYTCaC/Ay/zqNUzq//yJEjpKSk0KFDB9cyDoeD/Px8UlJSGDFiBH5+fixatIhnnnmGevXqMX78ePr27etaR0nfTSYTkZGRbvvJarXicDhIS0sjOzubunXruuaHhIQQFhZWbN+C80qjWrVquQIVQP369fn1119dy77yyit8++23hIeHc8UVV5Cfn+/6433+z3z48GGmTZvGgQMHaNiwIQ0aNHD93HXr1uU///kPs2fPZuTIkdhsNm677TYmTJjwp/vq/J+/pH1/7hibvXv3Mm7cOHx9fenevXux9/v6+nLTTTfx6aef0q1bNxYtWsTdd99d4nauvfZarr32WsB5umzlypW89NJLBAcHM2jQIEwmE+3atSuTMTZF272cz1elCjZNmjQpNqC46Ly0Jxh0dV1W7TjBl78cY8yCzfx3dDcCfPXHSESqpujoaOrXr8+XX37pasvIyCA5OZnw8HB27txJ7969ue+++0hPT+eDDz4gISGB9evXX/I2IyIi8Pf35+jRo64jNllZWZw+XfI4pejoaI4cOYLdbneFm2PHjrnmv/jiixw9epRVq1YRHBwMQHx8fInrys/P58EHH+Thhx/mrrvuwmQy8fPPP7N06VIAkpOTsdlsvP7669jtdn766SfGjBlDo0aN/nRfnT+o9880adKEt99+m1tvvZUnnniCadOmFVvmtttu44477mDHjh3s3buXnj17cvz4cdf8vXv3csstt7Bo0SLXkI/69evz17/+la1bt/Lrr79eVE0VpXL8N6DQX/7yFz788EPWrVuH3W5nxYoVJCYmul0SaCSTycTzt8VSs5of+05mMuWL7UaXJCJimF69epGZmcns2bPJy8vjzJkzPPbYYyQkJGAymVi4cCGPPvooycnJBAcHExwc7Dq9U3QqKj09/aK2aTabuf3225k5cybHjx8nOzub559//oIDT3v37o3D4WDmzJnk5eXx888/s3DhQtf8jIwM/Pz8sFgs5ObmMnfuXHbt2uW6YsvPz4/s7GwKCgrIz88nJyfH9Zyko0eP8sILLwDO0HP06FGGDRvGDz/8gNlspmbNmgCEhYX96b4q2h8ZGRml3hfR0dFMnTqVzz77jCVLlhSb37x5c5o0acKECROIj4/Hx8fHbX7jxo1p2bIlTz75JNu2bSM3N5fs7Gy+//57EhMT6dOnT6lrqUiVKtgMHDiQp556iueff5527doxc+ZMpk+fTsuWLY0uzSUsyJcXBzlPi72//hDf/Hr8T94hIuKdgoODmTdvHomJiXTv3p3rrrsOs9nMG2+8AcDDDz9MgwYNuPHGG2nbti2LFy9m1qxZ+Pn5ERMTQ7t27ejWrRvff//9RW13/PjxNG7cmP79+9OvXz+io6NdYzfOV61aNebMmcMPP/xAhw4deOKJJ+jXr59r/rhx48jJyeGaa66hd+/ebNmyhZtvvpldu3YB0L59eyIiImjfvj2//fYbzz33HK+//jpxcXHcc889dOnShRo1arBr1y5iY2N58sknefrpp4mLi2PIkCHcdddd3HDDDX+6r2rUqEGfPn0YPHgwH374Yan3RY8ePbj77rt55pln+O2334rNv+2229i1axe33357sXkmk4m3336buLg4JkyYQMeOHenSpQv//ve/eeGFF+jcubNr2Y0bN5Z4H5ubbrqp1LWWFZPDU87jVBCbzcaWLVto06ZNuY5ZeHbZdmav2U9EkC9fjutOZIhfuW2rLFXU/pHSUX94nvLqk5ycHPbv30+jRo2KDZqUCytpTMeGDRto3ry56x4rGRkZtGvXjhUrVtCwYUMDq/V+lzvG5o8+B6X97FWqIzaVyYTrm9MiOoTkzDwe/WSrx4wDEhHxdnPnzmXKlCnk5OSQm5vLjBkzaNSokUJNFaFgU078rBZm/CUOX6uZb3ee5D/rDxpdkohIlfD000+Tnp5Ojx496NKlCwcPHuTf//630WVJBalUV0VVNjE1Q/j7DS345+fbmfLfX+ncOIJmNUOMLktExKvVrFmTWbNmGV2GGERHbMrZfdc0pHtMJLkFdsYu2EJugW6NLyIiUl4UbMqZyWTixdtbER7ky/bfz/Dyyl1GlyQiIuK1FGwqQFQ1f6beFgvAv1fvY93eUwZXJCIi4p0UbCpI35bR/KVDPRwOGP/xVtKy8o0uSURExOso2FSgfwy4kkY1gvg9LYfHP03SJeAiIiJlTMGmAgX6WnllcBusZhP/3fY7i3+6uGd/iIiIyB9TsKlgreuFktDH+TCxJz/7mUPJWQZXJCIi5eHgwYq5f9mBAwcqZDuVhYKNAUb2aEKHhuFk5tkY99FmCmx2o0sSEbkkTz75JHFxccTFxREbG0uLFi1cr+Pi4ti4ceNFr/Ovf/0rb775ZqmWHTBgAF988cVFb+NSDB06lJkzZ5Zq2X/961+u5zyVp1WrVvHAAw9ccP7EiRNp2bKlW5+0bt2aAQMGsGzZMrflmjdvzksvvVRsHcnJyVx11VX07t3b1fb7778zYcIErrnmGtq0aUOvXr145plnOHPmjGuZoUOHctVVV7ltu+ir6Inn5UE36DOAxWzi5cGtueGV1fx0KJXXv93L2OuaGV2WiHgihwPyK/jIrk8glPI5P5MnT2by5MkALF68mNdee41Vq1Zd1uZnz55d6mWXLVtGVpbnHfk+ffp0hWwnNTX1T8drxsfHM3XqVNfrnJwc3n77bSZMmEDLli1p1KgR4HzK+GeffUZCQgJm89njHp9++qnr6eIAdrudYcOG0aVLF7788kuqVavGb7/9xuOPP87YsWN5/fXXXcs++OCDjB49uqx+3FJRsDFI3bBAnrnlKsZ9tIUZq3bTLaYGbeuHGV2WiHgShwPm9oPfEit2u/U6wbAvSx1u/sjhw4e59tpruf/++1m0aBEDBgzg73//O9OnT+e7777j2LFj+Pv7079/fyZNmoTJZGLo0KF06NCB0aNHM3HiRHx9fTlx4gSJiYmEh4dz7733cs899wDQu3dvRowYwZ133snQoUNp06YNP/30E9u3byc6OprRo0fTv39/Vy1PPfUUmzdvJioqijvvvJPnn3+enTt3llj7woULefPNN0lJSaFv375kZ2e75mVkZDB16lR+/PFHTpw4QUhICEOGDGHkyJG8/vrrfP755wBs376dpUuX8tNPP/HKK6+wb98+0tLSaNasGU8++SRt2rShoKCAZ599lq+++oqCggKaNGnC+PHjadeuHQDr1q3j5Zdf5sCBA9SsWZMHH3yQm266icTERJ566iny8/OJi4vjyy+/pGbNmn/aJ/7+/gwbNozXXnuNnTt3uoJNly5dWLduHevWraNr166u5RcvXsyNN97ImjVrAGdo27dvH88995zrQaP16tVj0qRJvPvuu9hsxt6IVqeiDHRLXB1ual0bm91BwkdbyMgtMLokEfE4lx8uPEFmZiZr164lISGBd999l9WrV/Puu++yefNmZs2axYIFC1i/fn2J7128eDFDhw5lw4YNDB8+nKlTp3L8+PESl/3444954oknSExMpG/fvjz55JPk5uZis9l48MEHiYqKYs2aNcyZM4dPP/30gvX+8MMPTJ48mWeffZYNGzbQunVrkpKSXPNffPFFDh8+zCeffMLmzZuZNGkS06dP5+DBg4waNYr4+Hji4+NZunQpOTk5PPTQQ/Tr14///e9/JCYmUr9+faZNmwbAZ599xubNm1m+fDnr1q2jffv2/POf/wRgx44dPPTQQ4wYMYLExESeeeYZnnvuOVavXk3Hjh355z//Se3atdm8eXOpQg04Q9kbb7xBSEgIbdu2dbX7+PgwYMAAlixZ4mrbsmULDoeDNm3auNoiIiLo1KkTf/vb35g6dSpff/01J0+epHnz5kyZMqVMn3p/KXTExmDP3HIVmw6e5mByFpM//4Vpt7c2uiQR8RQmk/PIiQefiiqtW265BV9fX3x9fbnjjju49dZbiYiI4MSJE+Tk5BAUFHTBsNKxY0e6dOkCwMCBA3nqqac4dOhQiX/I+/Xrx5VXXgnArbfeyptvvklycjK///47Bw4cYOHChQQGBhIYGEhCQgIjRowocZtLly6lb9++dO7cGYC77rqLhQsXuuaPHj0ai8VCcHAwx44dc52qOXHiBA0aNHBbl4+PDx999BENGjQgNzeXI0eOEBoa6gpK/v7+rpDUvXt3xo4dS0JCAgALFizg2muvpW/fvgC0bduWO+64g/nz59OtW7dS7ftly5bx9ddfY7PZyM/PJzAwkO7du/Phhx8SFRXltuzAgQMZPHgw6enphISE8MknnzBw4MBi63z77bdZuHAhX331FQsWLCA7O5sWLVq4HWkC+Pe//827775b7P2XMvaqtBRsDFY9wIeX7mjNX95ez8cbD9O7RRTXX1XL6LJExFOYTOAbZHQVl+3cP6DZ2dlMnjyZDRs2EB0dzZVXXonD4cBuL/lCisjISNe0j48PQKmWtVqtrmWPHTtGWFgYgYGBrvl169a9YL3Hjx+nZcuWbm316tVzTScnJzNlyhS2b99O3bp1ueqqqy5Yl8ViITExkeHDh5OVlUXTpk2xWq2usTE33ngj+fn5LFy4kJdffpmIiAhGjhzJX/7yF44cOcL69eu5+uqrXeuz2WzUr1//grWfb8CAAa4xNt9//z0TJkwgJiaGZs2Kj+1s0aIFjRs3Zvny5cTHx/PVV1/xxRdf8P3337st5+vry5AhQxgyZAg2m40dO3bwwQcf8NBDD/Hxxx+7wuWIESM0xqYq6tQ4gpE9mvDGd3uZuDiJNvXCiK7ub3RZIiJlxnTOEaBJkyZRvXp11qxZg5+fH3a7nfbt25fr9mvXrk1KSgrZ2dkEBAQAcPTo0QsuHx0dzW+//ebWduzYMVcYGDt2LL1792bOnDlYrVZOnz7Nxx9/XOK6tm7dyjPPPMOCBQtcAWju3Lns378fgP3799OyZUtuueUWcnJy+PLLL3nssce4+uqriY6O5tZbb3UN0AbnUaFLvcFrjx49eOGFFxg5ciTh4eHcfvvtxZa59dZbWbJkCb6+vrRv356IiAi3+R9//DGvvfYa3377LRaLBYvFQsuWLZkyZQpff/01e/bscQUbI2iMjYdIuC6G2DrVSc3KZ/zCLdjtuiuxiHinjIwM/Pz8MJvNZGRkMG3aNDIyMsjPL79HzbRu3ZqmTZsydepUsrOzOX78ODNmzLjg8gMHDuTrr7/m22+/paCggCVLlrB161bX/PT0dPz9/bFYLKSkpPDss88CuH4GX19f0tPTXcuazWb8/Z3/Yd2yZQvvvfceeXl5AHz77bf87W9/4/Dhw/j7+xMaGorVaiUkJITbb7+dZcuWsWbNGux2OwcOHODuu+9m7ty5APj5+ZGdnU1BQenHaPbo0YNhw4bxzDPPsHfv3mLz4+Pj+fnnn5k9e3aJwadnz57k5uby1FNPceDAAWw2G6mpqbzzzjsAbqeijKBg4yF8rWZeubMNAT4W1u5JZu7a/UaXJCJSLiZNmsSOHTvo0KED119/PRkZGXTr1o1du3aV2zbNZjMzZszgwIEDdO7cmXvvvZf27du7Tm2dr127dkybNo2pU6dy9dVXs2LFCtc4H4Dnn3+eL774grZt23LbbbdRs2ZNrrzyStfP0L9/f3766Sd69uxJly5duOuuuxgyZIhrYPDQoUNJSUnh1KlT3HPPPfTs2ZM777yTNm3a8MILLzB9+nSio6Np3bo1L7/8Mi+//DLt27fn7rvvpnfv3owfPx7AdUSlffv2F7y6qyTjxo2jSZMmPPLII66AVSQsLIxevXqRlpZW4jieqKgo17iaoUOHEhcXR79+/di8eTMffPABYWFnr/B96623SryPzblHoMqayVHFHlhks9nYsmULbdq0MXzkdknmJx7kiSU/42sx8+moLlxZu1qFbt/T909Vo/7wPOXVJzk5Oezfv59GjRq5/mcvf87hcJCVlUVgYKDb6a7z5eTksHnzZjp06ODqt1WrVvHUU0+xevXqiirX65W2Py7kjz4Hpf3s6YiNh7mrQ32uu6ImeTY74z7aTE6+sfcDEBHxBj4+PowbN46PP/4Yu91OcnIyc+fOpVevXkaXJmVMwcbDmEwm/jUwlhrBfuw6nsHU5TuMLklEpNKzWCy8/vrrLFmyhPbt2xMfH0+zZs2YOHGi0aVJGdNVUR4oItiPFwa14v53NjBv3QF6tYiiR0zkn79RREQu6Oqrr77glUviPXTExkP1ah7FvZ2dN3l6ZOFWkjNyDa5IRETE8ynYeLC/97+CZlHBnEzPZeLipEu+b4GIVB76nEtVVha//wo2Hszfx8Ird7bB12Lmq+3HWbDhtz9/k4hUSkWXHXvik6pFKkrR7/+FLsMvDY2x8XAta1dnQr/mTPniVyZ/vp2OjcJpHBlsdFkiUsYsFguhoaGcOHEC4JIvl61qHA4Hubm5mM1m7S8PcKn9UXSZ+IkTJwgNDb2sWyko2FQCD3RtxHe7TrB2TzLjPtrCooeuwceig20i3iY6OhrAFW7kzzkcDvLz8/Hx8VGw8QCX2x+hoaGuz8GlUrCpBMxmEy8Oas31r6xm2+E0Xvl6FxP6tTC6LBEpYyaTiVq1ahEVFVWujxfwJkUPYGzatKluYukBLqc/fHx8yqQPFWwqiVrVA3j+tlj+b/5PzPpuLz1ioujQKNzoskSkHBQ9WFD+nM3mvIlp0XObxFie0B86n1GJ9I+txe3t6uJwQMJHWziTo//RiYiInEvBppJ5+qaW1A8P5EhqNk9++rPR5YiIiHgUBZtKJtjPyvTBbbCYTXy65SifbTlidEkiIiIeQ8GmEmrXIIy/9WoKwKRPf+bwad33QkREBDwg2KSkpNCnTx8SExPd2jdv3kxsbOwF37d27VquuOIKDh8+XN4leqTRvZsSVz+U9JwCHv5oKza77lYqIiJiaLDZtGkTgwcP5tChQ642h8PBJ598wrBhw8jLyyvxfSdPnuSxxx7DbrdXVKkex2ox8+rgOIJ8Lfx4IIU3v99rdEkiIiKGMyzYLFmyhEceeYSEhAS39scff5yFCxcyZsyYEt9nt9t55JFHGDRoUEWU6dHqRwTy9E0tAZj+1S62HU41tiARERGDGXYfm65duxIfH4/VanULN2PHjiU6OrrYqakis2bNIiIigoEDBzJr1qxL3n7RtfaV3a1tarFqx3GW/3ycsQs2s3TUNQT6Xnq3Fu0Xb9k/lZ36w/OoTzyL+sOzlGd/lHadhgWbyMjIEtv/6FbKP/74I0uXLmXx4sWkpqZe1vaTkpIu6/2eZHATB4l7zew/lcX4/6zlwXbVL3ud3rR/vIH6w/OoTzyL+sOzGNkflebOwykpKUycOJHp06cTHBx82cEmNjbWq+5S+UpEMvfM3cDKfdkMvOYKrrsi6pLWY7PZSEpK8rr9U1mpPzyP+sSzqD88S3n2R9G6/0ylCTarV68mOTmZBx54AMA1cPimm25i5MiRjBgx4qLW5223LO8eE8Xwbo14e/V+/r7kZ+IadCMqxP+S1+dt+6eyU394HvWJZ1F/eBYj+6PSBJubb76Zm2++2fX68OHDXHvttSxdupS6desaWJnneKRfc9bsSebX388wYeE25t3fXk+7FRGRKsXw+9hI2fGzWnj1zjb4Wc18v+sk7647YHRJIiIiFcojgs3OnTvp2LGjW1vHjh3ZuXPnBd9Tt25ddu7cqaM154mpGcLj/a8A4LnlO9h1PN3gikRERCqORwQbKVv3dG5Az+aR5BXYGfPhZnILdBmkiIhUDQo2XshkMjHt9laEB/my41g6L6648JEvERERb6Jg46WiQvyZNrAVAG+v3s/aPacMrkhERKT8Kdh4seuurMldHesDMP7jraRmlfzsLREREW+hYOPlJt14BY1rBHHsTA6PL0nC4dBTwEVExHsp2Hi5QF8rr94Zh9Vs4oukY3yy6bDRJYmIiJQbBZsqILZudRL6xADw9NJfOJicaXBFIiIi5UPBpooY2aMJHRqFk5lnY9xHWyiw2Y0uSUREpMwp2FQRFrOJ6YPbEOJvZfOhVGau2mN0SSIiImVOwaYKqRMawLO3XAXAzFW72XTwtMEViYiIlC0Fmyrm5jZ1uKVNbewOSPhoCxm5BUaXJCIiUmYUbKqgybdcRZ3QAA6lZPH00l+MLkdERKTMKNhUQdX8fZg+uA1mE3yy6TBfJP1udEkiIiJlQsGmiurQKJyHejYB4O+Lk/g9LdvgikRERC6fgk0VNu66GFrVrU5adj7jP96K3a67EouISOWmYFOF+VjMvDK4DQE+FtbtTWb2mn1GlyQiInJZFGyquMaRwTwZfyUAL6zYyfajZwyuSERE5NIp2Ah3tq9Hnytrkm9zkPDxVnJtOiUlIiKVk4KNYDKZ+NfAVkSG+LHnZCbvbU03uiQREZFLomAjAIQH+fLioNYAfLk3i3vf2cDPR9IMrkpEROTiKNiIS4+YSB67vjlWE6zZk8yAmWtI+GgLh09nGV2aiIhIqSjYiJsR3Rox44Ya3NS6FgBLNh+h94vfM+W/20nNyjO4OhERkT+mYCPF1AyyMv2O1nz+t65c0ySCPJudt1fvp/u0b3nr+73k5NuMLlFERKRECjZyQbF1qzP/rx2Zd397WkSHcCangOeX76D3i9+xaNNhbLqhn4iIeBgFG/lDJpOJns2j+O+Ybrw4qDW1q/tzNC2H8Qu3cuOM1Xy38wQOhwKOiIh4BgUbKRWL2cTt7eqy6pGe/P2GFoT4W9lxLJ373tnA3XMSdQWViIh4BAUbuSj+PhYe7NGE1Y/2Yni3RvhazKwtvIJq7ILN/JaiK6hERMQ4CjZySUIDfXnixiv5ZnwPbo2rA8BnW45y7Uvf88yy7ZzO1BVUIiJS8RRs5LLUCw9k+uA2LBvdla5Na5BnszNnzX66v/Ats77boyuoRESkQinYSJm4qk513v9rR94b1oEralUjPaeAaV/upNeL37Fw42+6gkpERCqEgo2Uqe4xkfx3dFdevqM1dUID+D0thwmfbOPGGav5VldQiYhIOVOwkTJnNpu4rW1dvhnfg8f7t6Ba4RVU97+zgbveTmTb4VSjSxQRES+lYCPlxt/HwojuTfjfo70Y0b0xvlYzP+xL5qbX1jL6w80cStYVVCIiUrYUbKTchQb68nj/K1g1vge3xdXBZILPtx7l2pe/45+f/0KKrqASEZEyYniwSUlJoU+fPiQmJrq1b968mdjYWLc2h8PB66+/Tu/evWnbti3x8fF8+eWXFVmuXIa6YYG8PLgN/x3dje4xkeTbHLyz9gA9pn3L69/uITtPV1CJiMjlMTTYbNq0icGDB3Po0CFXm8Ph4JNPPmHYsGHk5bn/T/7dd99l8eLFvP3222zatImEhAQeffRRtm3bVtGly2W4snY13hvWgfcf6EjL2tVIzy3ghRXOK6g+3qArqERE5NIZFmyWLFnCI488QkJCglv7448/zsKFCxkzZkyx95w5c4ZRo0bRpEkTTCYTvXv3pkmTJvz0008VVbaUoa7NavD537ryyuA21AkN4NiZHB5dtI0bXv0fq3Yc1xVUIiJy0axGbbhr167Ex8djtVrdws3YsWOJjo4udmoKKBZ29u7dy+7du2nZsuVFb99m02mPkhTtl4rcP/Gtoul7ZRTvrz/I69/tY9fxDIbN20jHRmE81q85reuFVlgtnsaI/pA/pj7xLOoPz1Ke/VHadRoWbCIjI0tsj46OLtX79+/fz/Dhw7npppto3779RW8/KSnpot9TlRixf9oFw8y+YSzekcl/d2eSuP80t725nmvq+jMkNpjoYMN+XQ2n31fPoz7xLOoPz2Jkf1TKvxSrVq1i4sSJ3HbbbTz22GOXtI7Y2FgsFksZV1b52Ww2kpKSDN0/XTrA+NRsXvlmD4s3H2Hd4Rx+PJrLXR3r8bdeTYkI8jWkLiN4Qn+IO/WJZ1F/eJby7I+idf+ZShdsXn/9dWbPns3kyZOJj4+/5PVYLBZ9CP6A0funXkQwL93Rhr92a8y/vtzBdztP8t4Ph1j801FG9mjMsK6NCPStdL++l8zo/pDi1CeeRf3hWYzsD8Mv974Y77zzDu+88w7z58+/rFAjlccVtaox7/4OfPDXjsTWqU5GbgEvrtxFzxe+Y8GPhyiw2Y0uUUREPEilCTZF97DJzs5myJAhxMXFub7efPNNo8uTcnZN0xp8NqoLr97ZhnrhAZxIz2Xi4iSuf3U1X23XFVQiIuLkEcfyd+7cWaytY8eObu0mk4mNGzdWZFniYcxmEze3qcP1V0Xz/vpDzFy1mz0nMhj+3kY6NAzn7/1bEFc/zOgyRUTEQJXmiI1IET+rhQe6NuL7Cb14qGcT/KxmfjyQwq2z1vF/8zex/1Sm0SWKiIhBFGyk0qoe4MNj17fguwk9GdSuLiYTfJF0jD4vf8+Tn/3MqYxco0sUEZEK5hGnokQuR63qAbwwqDUPdGvEv5bv4NudJ3nvh4N8kHiI+uGBNIgIpEFEEI1qBNEgIpCGEUHUCQvAx6JcLyLibRRsxGu0iK7GO/d34Ie9yUxd/itbD6ex71Qm+05lAifdlrWaTdQNC6BBRBANIwJpWCOIhhHO4FMvPFChR0SkklKwEa/TuUkEn47qwu9pORxIzuTAqSwOJme6pg8kZ5JbYOdAchYHkrP4/rz3W8wm6oQGuI7uOEOP86hPvfAA/Ky6V4aIiKdSsBGvZDKZqB0aQO3QAK5p4j7PbndwIj2X/acyCwOPM/g4X2eRnW/jUEoWh1KyWL37lNt7zSaoHRrgOrrjPL3lDD71wgPx91HoERExkoKNVDlms4no6v5EV/enc5MIt3kOh4OTrtDjPLpz7lGfzDwbh09nc/h0Nmv2uK/XZILa1QPOGdMTWBh6nCFIoUdEpPwp2Iicw2QyEVXNn6hq/nRsXDz0nMrIKww6zuCzP7nwqM+pLDJyCziSms2R1GzW7U0utu5a1f2LHeUpCj4Bvgo9IiJlQcFGpJRMJhORIX5EhvjRvmG42zyHw0FKZp7b0Z3955ziSs8p4Pe0HH5Py2H9vpRi665Zzc95lCciiAY1As8OZA71r6gfT0TEKyjYiJQBk8lERLAfEcF+tGtQPPSczsrnwDlHdw6cM7YnNSuf42dyOX4mlx/3Fw89oX5mGq7/gTphgdQNDaBOWAB1zvke4u9TUT+miIjHU7ARKWcmk4nwIF/Cg3xpW8IjH1Kz8ooNYC463XU6K5/UXDtbfktjy29pJa6/mr+VOmGB1AkNoO55oadOWAARQb6YTKby/jFFRDyCgo2IwUIDfWkT6EubeqHF5qVk5PD1+i0ER9XjaFqucwzP6WzXWJ7UrHzO5BRw5vcz/Pr7mRLX7+9jpnZoQAnBJ5A6YQHUDPHDqvv2iIiXULAR8WDVA3xoEuZDm5bRWCzFBxhn5BZwtDDsHD439JzO4khqNifSc8nJt7PvZCb7Tpb8DC2L2UR0NX/qhAWUeKqrdmiArugSkUpDwUakEgv2sxJTM4SYmiElzs8rsPN7WknBx/n997Rs8m0O1xGgHy+wnRrBfsWDT2HoqRMWQPUAjfMREc+gYCPixXytZhpEOC8vL4nN7rxvz5HULA6fF3qKvmfl2TiVkcupjFy2/pZa4npC/KzFjvSc+z0y2E/jfESkQijYiFRhlnNuVtiuQfH5DoeD1Kx8jqRmnxd8slzTp7PySc8tYMexdHYcSy9xO75WM3VDA2hYw/kw0kY1gmhcI4hGkUHUDPHHbFboEZGyoWAjIhdkMpkIC/IlLMiXq+pUL3GZzMJxPiWd6jpyOpvj6TnkFdjPeSCpuwAfCw0iAmkcWRR6gl3BJyzIt7x/RBHxMgo2InJZgvysNKsZQrM/GOdzLC2H305nsf+U8zL2/YVfh1Kcz+a60NGe0EAftyM85x7xCfTVP18iUpz+ZRCRcuVrNVM/IpD6EYF0aVrDbV6+zc7h09nsP5XBvpNnA8/+U5n8npZDalY+mw+lsvlQarH1Rlfzd4acyMLTWoVf9cID8dHl6yJVloKNiBjGx2J2BZLeLdznZefZOJB8Nug4g08G+wtvXHjsTA7HzuTwwz7353JZzCbqhzufydUwwj34RFfTeB4Rb6dgIyIeKcDXwhW1qnFFrWrF5p3OzGN/cib7Tzqfvr7vlHN6/6lMsvNtrjB0Pn8fMw0jgjSeR8SLKdiISKVTNKD5/EdUOBwOjp/JZV/hkZ39J93H8+Tk2y84nqd6gM/Zq7UKT3FpPI9I5aNPq4h4DZPp7OXr1zQpPp7nyOls52mtU2dPa+0/mcnRtBzSsvPZ8lsqW0q4V8+543kahgdgSs8lskE2dcODdGpLxMMo2IhIleBjMdOw8MqqXufNO388z7lfKZl5JY7nmbLmewJ9LTSNCqZZVAjNagbTrHC6bliAAo+IQRRsRKTK+6PxPKlZeW5BZ++JDH4+dIrfM21k5dnYdjiNbYfdn7zu72N2BR7n92Ca1QyhfnggFgUekXKlYCMi8gdCA32Jq+9LXOF4HpvNxpYtW2gZ24rDqbnsOZHO7uMZ7DqRwe7j6ew7mUlOvp2fj5zh5yPuT1z3tZppEll0ZCe48P4/wTQID9QT1kXKiIKNiMgl8LE4j8o0jQrm+qvOthfY7Px2Opvdx9PZXRh2dp/IYM+JDHIL7Pz6+xl+/f28wFN42XvTmsHEnHNaq0FEEL5WBR6Ri6FgIyJShqzn3Junb8uz7Ta7gyOns9lVFHhOpLPnRAa7j2eQnW9j5/F0dh5P57/8fnZdZhMNawQRUzOYplEhhUd5nJeo+1ktBvx0Ip5PwUZEpAJYzCbXHZivu7Kmq91ud3AkNdsZcs45rbXneDqZeTb2FB7tgWNu62oQEegarOw8whNC48gg/H0UeKRqU7ARETGQ2WyiXngg9cID6dUiytXucDj4PS3n7Oms42eDT3puAftOOu/GvOKX42fXZYL64YE0jQohpmawK/A0iQwmwFeBR6oGBRsREQ9kMpmoHRpA7dAAesREutodDgcn0nOdp7SOZxSO30ln1/EM0rLzOZCcxYHkLL7+9fg564K6YQHERIXQtObZq7UignwJ9rMS5GfVWB7xGgo2IiKViMlkomY1f2pW86dbM/fAczIjlz2FYWd3YdjZfTyd01n5/JaSzW8p2Xyz40SJ6/W1mAnysxDkZ3WFHee0hSBfq1t7sJ+FYH8rQb7nL2t1rsPXqvv4iGEUbEREvIDJZCIqxJ+oEH+uOe8p6skZuYVh5+xprb0nnUd4cgvsAOTZ7ORl2TmdlV8m9QT6WoqFnWKBqVQhyoq/jxmTSUFJSkfBRkTEy0UE+xER7EenxhHF5uXb7GTl2sjIKyAzt4CMXOd357Ttgm2ZeQWk55zbXkBmng2b3QFAVp7zBoYn03Mvu36L2USgr6XEEBToa+FMahoR+5Iwm8yYTM5Tb+AMQiaTc6ooF5kwndN2Niw524q/p2iZolWWuMx5bZzznqL5Z7fvvt5zt202gdnkXNZiNmE2FbadO20qnDafM20yYTE711lsuvB9JpOpcJ3u0yWvj8L1OGs5d9pyzjbd3lM47SjsfyMZHmxSUlIYPHgwzz77LB07dnS1b968mXvuuYekpCS35ZcsWcKsWbM4efIkjRs35h//+AdxcXEVXbaIiFfwsZipHmimeqDPZa/L4XCQW2B3BSHnd/dw5GrLOz8wFV82M88GOC+VT89xBqkLOnDksuuXshEX7cvC1sYFHEODzaZNm5g4cSKHDh1ytTkcDhYtWsSUKVPIy8tzWz4xMZFnnnmGt99+m1atWjF//nweeughvv32WwICAiq6fBEROYfJZMLfx4K/j4UawX6XvT673UFW/gWCUeF0ek4+h48coVatWphMZhwOB47Cv6kOwOEAB84G57RzoujPbtH8Et/j1lbyMo5z/n47LrDec9s4d1vnLeNwgN3hKPxy/vxFr2125/rtDgc2xznTdueyjnOmXeuwX2h9zrDoKJp2OIq///x1FU6f+/O6cxBEDhGmM+TmRl1ooQphWLBZsmQJM2bMYMKECSQkJLjaH3/8cfbt28eYMWOYOnWq23sWLlzIjTfeSLt27QC47777+Oijj/jiiy8YOHBghdYvIiLly2w2EVw4zqbmBZZxPuIijTZtmmCx6JL2MpWfA5knIesUZJ6CzJM4Mk7iyDwJmadwZJ7ElHkKsk5iykrGVJADQLa5LiauN6xsw4JN165diY+Px2q1ugWbsWPHEh0dTWJiYrH37Nmzp1iAadq0KTt27Ljo7dtstosvugoo2i/aP55B/eF51CeeRf1xEewFkJXsCimmrGRncCkKJ5mn3EKMKS+j2CpMFI1eKpnDGkBmWEssdjuUcZ+Uto8NCzaRkZEltkdHR1/wPZmZmcVOOfn7+5OVlXXR2z9/7I640/7xLOoPz6M+8SxVsj8cdiz5GVhzU/HJO134PRVrbirWvFR8ck+fM52KNf/Mn6/zPHaTlQK/6hT4hpHvF0qBbygFfmHk+1anwC+MAr9Q8n1DKSicZ7cW/o02sD8MHzx8MQICAsjJyXFry8nJISws7KLXFRsbq8OWJbDZbCQlJWn/eAj1h+dRn3gWr+oPhwPyMlxHUMhMxpR10nUEhcxTmIqOqBR+Nzku7qiIAxMERkBQDQiKxBEYAUGREFj4OqhG4bTzNX7VsJhMWIDSjJoqz/4oWvefqVTBplmzZuzevdutbc+ePXTv3v2i12WxWCr/h6Acaf94FvWH51GfeBaP7o+8LMg47gwnGccLv04U/555Egpy/nx95/Ovfk44KQwkQZGF0+e+jsQUEAbms/upvO4OZGR/VKpgc/vttzNq1ChuuOEG2rVrx/z580lOTqZPnz5GlyYiIlWJLf+coHJuSCkhsOSlX9y6fQKLB5TACwSWwBpg9S2fn7GSqlTBpnPnzjz11FM8/fTTHD9+nKZNm/L2228TGhpqdGkiIlLZ2e2QfbqEoyrHi4eYrOSLW7fVH4JrFn5FnfO9cDqocDqoBvgGlc/PV0V4RLDZuXNnsbaOHTuW2H7zzTdz8803V0RZIiJS2RWNWzk3pBSbLvzKPOG8cqi0TBb3cOL6XtN5NOXcIOMXcvb2x1KuPCLYiIiIXBRbvjOYpB2l+rH1mH7a4hxUe354yTwJ+Rd55WxAeAlHVopCyjmBJSAczHoquqdRsBEREc9RFFjSjxV+/e78nlH0+rizLSsZcGABmpZmvb7BJQSVKPejKkFRziMtGrNSqSnYiIhI+SvIOxtYXCHlWPHXWadKv06zFUdwFFnmEAIjG2EKOX8MyznjWDRupcpQsBERkUtXkHveEZYLBJeLGWxr9nGGkpDos1/B0e6vQ2pBQDh2h4MdW7bQpk0bz73cWyqUgo2IiBRXkFsYSgpP/RSdAjr/dXZK6ddp9ikMKTXPhpOQms7v5waXixm7okcpyHkUbEREqhK7rXDQ7WE4c/TCwSX7dOnXafY5J6SUcHQluOgIS5gG20q5U7AREfEmeZnO0JL6G6T95pxOO1w4/ZszzJT2kmaL7zkhpejISk33Iy1FgUWXMouHULAREaks7HbnvVaKgkpqCcGlNEdaTBaoVrswmJQ0jqWwXYFFKiEFGxERT5GffU5IOXxOcCn8fuYI2PL+fD1+1aB6Pahe9+xXaP2z08HRYNE//+Kd9JstIlIRHA7nDeTSDp1zlOUwpJ7zujSXOpvMziMqfxRc/KuX/88j4qEUbEREykJ+jvOIyrlHWM498nLmSOme3OwTBKH1zgsu9Qrb6jpDjcWn/H8ekUpKwUZEpDQcDmdQObqNqH3rMJ385GyQSf3NOfblT5mcY1eKwsq534uCi3+oxrWIXAYFGxGR8zkcztBydAsc3ez8+n0LZCVjAepd6H0+ge5HWc496hJaD0Jq63b9IuWs1MHm2LFjREdHX3D+8uXLueGGG8qkKBGRCuNwOO/bcnTz2SDz+xbnwxPPZ7biiGxBqjmc6g1iMZ87riW0vq4iEvEApQ42/fv356effnK9HjlyJG+++abr9RNPPKFgIyKeL/2Ye4g5urnk00gmC0RdCbXbFH7FQVRL7GYf9hXewh/dwl/E45Q62DgcDrfX54ackuaLiBgu/bjz6Mu5QSbjWPHlTBaIbOEML0UhpmZL8Akovqxu4S/i0UodbEx/cnj1z+aLiJSrjJPFQ0z60eLLmczOEFOrzdkgU/Mq8A2s2HpFpFxo8LCIVD6ZyfD75nNCzBY4c7iEBU0Q2dw9xETHgm9QhZYrIhVHwUZEPFtWytkBvUc3w9GtzpvcFWOCGs2cAaYoyETHgl9wBRcsIkYqdbCx2+1s3LjRNZamoKDA7bXdbi+fCkWk6sg+7Tz64goxm5135i1JRFP3EFOrFfiFVGCxIuKJSh1scnJyuPvuu93azn2tMTYiclGyU+H3re5HY04fKHnZ8CZnB/XWauMMMXpsgIiUoNTBZseOHeVZh4h4s/xsOLzRPcSk7Ct52bBG54WY1hAQWnG1ikildlljbBwOB2lpaYSGhpZROSLiFex2OLYV9n0He7+FQ+vBllt8udAG7pdY12rtvMmdiMgluqhg8+abb2K1WvnrX//KwYMHuf/++/n9999p3749s2bNIjhYg/REqqzTB84Gmf3/g+wU9/khtaBue/ejMYHhFV+niHi1Ugebd955hw8++IDHH38cgClTplC7dm3efPNN3n77bWbOnMnf//73citURDxMVgocWO0MMvu+g9P73ef7hkCjbtC4FzTu6bxiSWPxRKSclTrYLF68mJkzZ9K6dWsyMzNZt24db7/9NjExMSQkJHD33Xcr2Ih4s4Jc+C3xbJA5uhk4547jZqvziExRkKnTDiy6o4SIVKxS/6tz5MgRWrduDUBSUhIAbdu2BaB27dqkpKRc8L0iUgnZ7XDil7NB5uA6KMh2Xyayxdkg07CLLrcWEcOVOthYLBYKCgqwWq1s2bKFFi1a4OfnB8CJEydc0yJSiaUdPhtk9n0HWafc5wfXPBtkGveEarUqvkYRkT9Q6mATGxvL8uXLuf766/nvf/9Lnz59XPNWrlxJy5Yty6VAESlHOWmwf3VhkPkWkve4z/cJch6JadwLmvRyHqHROBkR8WClDjajRo1i2LBhTJ48GX9/f9fN+caNG8eqVat46623yq1IESkjBXlweMPZIHNkEzjOuWu4yeIcG1N0RKZue7D6GlSsiMjFK3WwadeuHcuWLePnn3+mY8eOhIc7L9P09fXltddeo3PnzuVWpIhcIocDTvx6NsgcWAv5me7LRDRzhpgmvaBhV93RV0QqtYu6ZKFevXrUq1fPrW3atGllWpCIXKYzR2Hf984gs+87yDjuPj+wxtkjMo17Qmi94usQEamkSh1sSnMp9/PPP39ZxYjIJchNdx6JKQoyJ897/Ik1ABpcc/aoTFRLMJuNqFREpNyVOtgsWbKEkJAQunXrpiugRIxky4cjP50NMoc3gL3gnAVMzjv7Nim8eqluB/DxN6hYEZGKVepgM2PGDBYvXsy6devo378/AwcO1JVQIhXB4YBTu+DA/5xBZv9qyEt3Xyas0TnjZLrpUQUiUmWVOtj07duXvn37cvLkSZYsWcLDDz9MQEAAt99+O/Hx8VSvfvEDDlNSUhg8eDDPPvssHTt2BGDr1q08++yz7Nmzh7CwMB566CEGDRoEgN1u59VXX2XJkiVkZmbSpEkTHnnkETp06HDR2xbxeBknMa1/g9iN72HJOek+LyAMGvU4e1QmrKERFYqIeJyLvt95ZGQkI0aMYMSIEWzcuJHFixcza9YsOnfuzEsvvVTq9WzatImJEydy6NAhV1taWhojRoxgzJgxDB48mA0bNjBq1CiaN29Oq1atWLBgAV9//TULFy4kMjKS9957jwcffJD169fr9Jh4j9MHYd1M2PwfzAU5+AIOix+m+p3OBpno1honIyJSgst6kEtkZCRRUVH4+/uTmJhY6vctWbKEGTNmMGHCBBISElztK1euJDQ0lCFDhgDQuXNn4uPjmT9/Pq1atWLfvn3Y7XbsdjsOhwOTyYS//6WNHbDZbJf0Pm9XtF+0fwxwYjumdTMw/bwIk8O5/+214thf+2bq9R6GxT/47LIOB6iPDKHPiGdRf3iW8uyP0q7zooNNRkYGy5cvZ9GiRWzfvp2ePXvyj3/8g+7du5d6HV27diU+Ph6r1eoWbHbv3k1MTIzbsk2bNuWTTz4B4M477+Sbb76hZ8+eWCwW/Pz8+Pe//31JR2uKnnclJdP+qThBKT8TvedDQo//4Go7U6Mdx5r9hfSIODCZSN2x5w/WIEbQZ8SzqD88i5H9Uepgs3btWhYvXsw333xDo0aNuPXWW3njjTcICwu76I1GRkaW2J6ZmUlAQIBbm7+/P1lZWQDk5+fToUMHHnzwQWrXrs2cOXMYM2YMS5cuveA6LyQ2NhaLxXLRtXs7m81GUlKS9k95czhg79eY176C6ZAz0DgwwRXx2LuMI6hWG5qg/vBE6hPPov7wLOXZH0Xr/jOlDjYPPPAA4eHhDB48mCuuuAKA77//3m2ZW2655eKqPE9AQADp6e5Xe+Tk5BAUFATAo48+ysiRI2ncuDHgfMzDZ599xpdffsnQoUMvalsWi0Ufgj+g/VNObAWw/VNY8wocL/yAmn2g9Z2YuoyDGk0paa+rPzyP+sSzqD88i5H9UepgU7t2bQC++uorvvrqq2LzTSbTZQebmJgY1q5d69a2Z88emjVrBsDRo0fJy8tzm2+1WvHx8bms7YqUu/wc2PoBrH0VTh9wtvkEwdX3Q+dRUK22oeWJiHiLUgebVatW4XA4SEtLIzQ01G1ebm5umTxaoU+fPrzwwgvMmzePIUOGsGnTJj7//HNmzZoFQO/evXnjjTe4+uqriY6O5oMPPuDkyZP06tXrsrctUi5yzsDGOfDDLMg84WwLCIdOD0H7v+p+MyIiZazUwWbHjh2MGjWKo0eP0qpVK/79739TvXp1du7cyfjx4zl+/Dj/+Mc/LquYsLAw5s6dy5QpU5gxYwbh4eFMmjSJTp06AfD0008zffp0hgwZQnZ2Ns2bN2fOnDnUrFnzsrYrUuYyTsD6N2DDHMhNc7ZVqwvXjIa2Q8E3yNj6RES8VKmDzbPPPktMTAz/+Mc/+M9//sObb75Jjx49+L//+z+aN2/Om2++eUkF7Ny50+11bGwsCxYsKHHZoKAgJk2axKRJky5pWyLl7vSBwnvQvA8FOc62Gs2h6ziIHQQWnTYVESlPpQ42v/76K1999RXh4eG0aNGCu+++m0WLFnH33Xczbtw4zLpZmFRlx39xDgj+eREU3oOGOldDt4ch5gbdTE9EpIKUOtjY7XbCw53jAaKjozl27BgPP/www4YNK7fiRDzeofWw+mXYveJsW5Pe0PVhaNgVTCbjahMRqYJKHWxM5/0D7ePjc9GXWIt4BYcDdn8Fa16GQ0U31TNBy1ugyzio3ca42kREqrhLfqSCj4+PLrOWqsVWAL8sgTXT4cQvzjaLL7T+C3QZCxFNjK1PRERKH2wKCgr49NNPXa/z8/PdXsPl36BPxCPl58CW92HtDEg96GzzDXbeg6bTKKhWy9j6RETEpdTBpkaNGsyYMcP1OiwszO11WdygT8Sj5KQ5L9de/8bZe9AERkDHh6DDXyHg4h8nIiIi5euibtAnUiWkH4fEonvQnHG2Va8H14yBuLvBN9DY+kRE5IIueYyNiNdJ2X/2HjS2XGdbZAvomgBXDdQ9aEREKgEFG5FjPzsHBP+yGBx2Z1vd9s5LtmOu1z1oREQqEQUbqboO/uC8ZHv3yrNtTa9zHqFp0EX3oBERqYQUbKRqcThg1wrnEZrf1jvbTGa48hbnYw9qtTayOhERuUwKNlI12Aqcp5rWTIcT251tFl9oc5dzULDuQSMi4hUUbMS75Wc7BwOvmwGph5xtvsFw9TDoPApCoo2tT0REypSCjXinnDOw4e3Ce9CcdLYF1oBOI6G97kEjIuKtFGzEuzgczlNOX/4dMo4726rXhy5joM0Q3YNGRMTLKdiI90jZD/8dD3u/cb4ObwI9HoOrbtM9aEREqggFG6n8CvKcY2j+9wIU5IDFD7qNd17lZPUzujoREalACjZSuR38AZaNg5M7nK8bdYcbp0ONpoaWJSIixlCwkcopKwW+ehI2/8f5OrAG9HsOWt2hG+uJiFRhCjZSuTgcsHUBrHwCspKdbW3vheuehsBwQ0sTERHjKdhI5XFqNyxLgAOrna+jroQB06F+J2PrEhERj6FgI54vP8f5TKc108GWB9YA6PkYdP6brnYSERE3Cjbi2fZ9B8sehpS9ztfN+kL/FyCsoZFViYiIh1KwEc+UcdI5jmbbR87XwdFww7/gyps1OFhERC5IwUY8i90Om9+Dr56CnFTABB2GQ+9J4F/d6OpERMTDKdiI5zi+3Tk4+Lf1ztfRrSD+FajTztCyRESk8lCwEePlZcH3/4IfXgN7AfgEOY/QdBgBFv2KiohI6emvhhhr10r4YjykHnK+bjHAOZamel1j6xIRkUpJwUaMceZ3+PIx2P6Z83W1us6rnVr0N7YuERGp1BRspGLZbbBhDnwzGfLSwWSBTg9Bz7+DX7DR1YmISCWnYCMV5+gW5wMrj252vq7TDga8ArVaGViUiIh4EwUbKX+56fDtc5D4Jjjs4FcNrn0Srh4GZovR1YmIiBdRsJHy9esyWP4onDnifN3yNrj+eQiJNrYuERHxSgo2Uj5Sf3MGmp1fOF+HNYQbX4Km1xlaloiIeDezkRtPSUmhT58+JCYmutq2bt3KoEGDiIuLo3fv3ixcuNDtPStWrGDAgAG0adOGPn368Mknn1R02fJHbAWwbia83tEZasw+0G08/N96hRoRESl3hh2x2bRpExMnTuTQoUOutrS0NEaMGMGYMWMYPHgwGzZsYNSoUTRv3pxWrVqxfv16Jk6cyCuvvEL37t1JTExk+PDhxMTE0KqVBqAa7vBG+HwcHE9yvq7f2Tk4OKqFkVWJiEgVYkiwWbJkCTNmzGDChAkkJCS42leuXEloaChDhgwBoHPnzsTHxzN//nxatWrFvHnzuOeee+jRowcAnTp1YtGiRURFRV10DTabrWx+GC9TtF8uav/kpGFa9QymTe9gwoEjIAzHtU/jaDMETGbQvr5kl9QfUq7UJ55F/eFZyrM/SrtOQ4JN165diY+Px2q1ugWb3bt3ExMT47Zs06ZNXaebtm3bRseOHRkxYgRbt24lOjqa0aNHF3tPaSQlJV3eD+HlSrV/HA7Cjn5LvV9m4ZObAkBy3b4cvnIkBaZQ2LqtfIusQvT76nnUJ55F/eFZjOwPQ4JNZGRkie2ZmZkEBAS4tfn7+5OVlQU4T1XNmTOHmTNnEhsby6pVq0hISOD999+ndevWF1VDbGwsFosuNT6fzWYjKSnpz/dPyn7Myx/BtO9bABzhTbHf+BKhDbsRWjGlVgml7g+pMOoTz6L+8Czl2R9F6/4zHnVVVEBAAOnp6W5tOTk5BAUFAeDr68vAgQOJi4sDoG/fvnTu3JkVK1ZcdLCxWCz6EPyBC+6fgjxYNwP+9wIU5IDFD7qNx9R1HBarX8UXWkXo99XzqE88i/rDsxjZHx4VbGJiYli7dq1b2549e2jWrBkATZo0IS8vz22+zWbD4XBUWI1V2sF1sCwBTu5wvm7UAwZMh4gmxtYlIiJSyNDLvc/Xp08fTp06xbx588jPz2f9+vV8/vnnDBw4EIC//OUvfPjhh6xbtw673c6KFStITExkwIABBlfu5bJS4LNR8M4NzlATWANuexvu+UyhRkREPIpHHbEJCwtj7ty5TJkyhRkzZhAeHs6kSZPo1KkTAAMHDsRsNvP8889z+PBh6tSpw/Tp02nZsqXBlXsphwO2fggrJ0FWsrOt7b1w3dMQGG5oaSIiIiUxPNjs3LnT7XVsbCwLFiy44PK33nort956a3mXJad2wfIJcGC183XUlc7TTvU7GVuXiIjIHzA82IiHKcih1o53MH/xEdjywBoAPR+Dzn8Di4/R1YmIiPwhBRs5y+HA/NFd1N73nfN1s77Q/wXnc55EREQqAQUbOevAGkz7vsNu9sFx67+xXHUrmExGVyUiIlJqHnVVlBhszcsAnKrfH668WaFGREQqHQUbcTq6GfauwmGycLzJYKOrERERuSQKNuK0ZjoAjqsGkhcYbXAxIiIil0bBRuDUbti+FADHNWMNLkZEROTSKdgIrH0FcEDz/hB1hdHViIiIXDIFm6ou7Qhs/cg53fVhY2sRERG5TAo2Vd0Pr4E9Hxp2g3rtja5GRETksijYVGWZybBpnnO6a4KhpYiIiJQFBZuq7Me3ID8LarWGJr2NrkZEROSyKdhUVbnpkPiWc7rrw7oZn4iIeAUFm6pq0zzISYWIpnBFvNHViIiIlAkFm6qoIBd+eN053WUcmC2GliMiIlJWFGyqoq0fQvrvUK0OtNLjE0RExHso2FQ1dhusfdU53flvYPU1th4REZEypGBT1Wz/FFL2QUA4tLvX6GpERETKlIJNVeJwwGrnwy7pOBJ8g4ytR0REpIwp2FQle76G40ngGwwdhhtdjYiISJlTsKlKVr/s/N7uPggMN7QUERGR8qBgU1UcWg+H1oHF1zloWERExAsp2FQVRUdrWv8FqtUythYREZFyomBTFRz7GXavAJMZuow1uhoREZFyo2BTFawpvBLqylsgoomhpYiIiJQnBRtvl7IPflnsnO6aYGwtIiIi5UzBxtutnQEOOzTtA7VaGV2NiIhIuVKw8Wbpx2DLfOd0t4eNrUVERKQCKNh4sx9eB1se1OsEDa4xuhoREZFyp2DjrbJPw8a5zmkdrRERkSpCwcZb/Tgb8jKg5lXQrK/R1YiIiFQIBRtvlJcFiW84p7smgMlkbD0iIiIVRMHGG/30HmQlQ1hD571rREREqggFG29TkAfrZjqnu4wFi9XYekRERCqQocEmJSWFPn36kJiY6GrbunUrgwYNIi4ujt69e7Nw4cIS37t27VquuOIKDh8+XFHlVg5JC+HMYQiuCa3vMroaERGRCmVYsNm0aRODBw/m0KFDrra0tDRGjBjBLbfcwoYNG5gyZQrPP/8827Ztc3vvyZMneeyxx7Db7RVdtmez22HtK87pzqPAx9/QckRERCqaIcFmyZIlPPLIIyQkuN/if+XKlYSGhjJkyBCsViudO3cmPj6e+fPnu5ax2+088sgjDBo0qKLL9nw7lsGpXeBfHa4eZnQ1IiIiFc6QARhdu3YlPj4eq9XqFm52795NTEyM27JNmzblk08+cb2eNWsWERERDBw4kFmzZl1yDTab7ZLf65EcDsyrX8YE2K/+Kw5rIFzCz1i0X7xu/1RS6g/Poz7xLOoPz1Ke/VHadRoSbCIjI0tsz8zMJCAgwK3N39+frKwsAH788UeWLl3K4sWLSU1NvawakpKSLuv9nibk5CZift+M3exHUmAXCrZsuaz1edv+qezUH55HfeJZ1B+excj+8KhLZgICAkhPT3dry8nJISgoiJSUFCZOnMj06dMJDg6+7GATGxuLxWK5rHV4EvN/nnZOXH0vV3XsecnrsdlsJCUled3+qazUH55HfeJZ1B+epTz7o2jdf8ajgk1MTAxr1651a9uzZw/NmjVj9erVJCcn88ADDwC4Bg7fdNNNjBw5khEjRlzUtiwWi/d8CA5vggP/A7MV8zVjoAx+Lq/aP15A/eF51CeeRf3hWYzsD4+6j02fPn04deoU8+bNIz8/n/Xr1/P5558zcOBAbr75ZrZu3crGjRvZuHEjS5cuBWDp0qUXHWq8zpqXnd9bDYbQesbWIiIiYiCPCjZhYWHMnTuXL7/8ko4dOzJp0iQmTZpEp06djC7Nc53Y4bwaChN0GWd0NSIiIoYy/FTUzp073V7HxsayYMGCP31f3bp1i723Siq6b80VAyAy5g8XFRER8XYedcRGLlLqIeedhgG6PmxsLSIiIh5AwaYyWzcT7AXQuCfUaWt0NSIiIoZTsKmsMk46n+INOlojIiJSSMGmskp8AwpyoE47aNTd6GpEREQ8goJNZZRzBn6c7Zzu+jCYTMbWIyIi4iEUbCqjjXMgNw0iW0Dz/kZXIyIi4jEUbCqb/Gz4ofDhn13GgVldKCIiUkR/FSubLfMh8wRUrw+xtxtdjYiIiEdRsKlMbAWwdoZz+prRYPExth4REREPo2BTmfyyGFIPQmANiLvb6GpEREQ8joJNZWG3w5rpzulOD4FvoLH1iIiIeCAFm8pi9wo4sR18Q6D9X42uRkRExCMp2FQGDgesftk53f4BCAg1tBwRERFPpWBTGRxcC4d/BIsfdPo/o6sRERHxWAo2lUHR0Zq4uyGkprG1iIiIeDAFG093dAvs/QZMFugyxuhqREREPJqCjacruhLqqoEQ1tDQUkRERDydgo0nO7UHtn/mnO6aYGwtIiIilYCCjSdb+wrggJgboOaVRlcjIiLi8RRsPFXaEdi6wDnd7WFjaxEREakkFGw81Q+vgz0fGnSFeh2MrkZERKRSULDxRFkpsGmec7qbxtaIiIiUloKNJ0p8C/IzIboVNLnW6GpEREQqDQUbT5ObAYlvOqe7PQwmk7H1iIiIVCIKNp5m0zzISYWIpnDFTUZXIyIiUqko2HiSglz44TXndJexYLYYW4+IiEglo2DjSbZ+COm/Q0htaHWn0dWIiIhUOgo2nsJug7WvOqev+RtYfY2tR0REpBJSsPEU2z+FlH0QEAZt7zW6GhERkUpJwcYTOBywuvBhlx1Hgl+wsfWIiIhUUgo2nmDP13A8CXyCoMMIo6sRERGptBRsPMHql53fr74fAsONrUVERKQSU7Ax2qH1cGgdmH2g8yijqxEREanUFGyMVnS0ps1foFptY2sRERGp5AwNNikpKfTp04fExERX29atWxk0aBBxcXH07t2bhQsXuuY5HA5ef/11evfuTdu2bYmPj+fLL780ovSycexn2L0CTGboMs7oakRERCo9w4LNpk2bGDx4MIcOHXK1paWlMWLECG655RY2bNjAlClTeP7559m2bRsA7777LosXL+btt99m06ZNJCQk8Oijj7rmVzprCq+EuvJmiGhibC0iIiJewJBgs2TJEh555BESEhLc2leuXEloaChDhgzBarXSuXNn4uPjmT9/PgBnzpxh1KhRNGnSBJPJRO/evWnSpAk//fSTET/G5UnZB78sdk53TfjjZUVERKRUrEZstGvXrsTHx2O1Wt3Cze7du4mJiXFbtmnTpnzyyScAjBkzxm3e3r172b17Ny1btrzoGmw22yVUXnZMa17F7LDjaHIt9qirwOB6ihTtF6P3jzipPzyP+sSzqD88S3n2R2nXaUiwiYyMLLE9MzOTgIAAtzZ/f3+ysrKKLbt//36GDx/OTTfdRPv27S+6hqSkpIt+T1mx5iQTu8V5FGpXzQFkbNliWC0XYuT+keLUH55HfeJZ1B+exdC/sYZtuQQBAQGkp6e7teXk5BAUFOTWtmrVKiZOnMhtt93GY489dknbio2NxWIx5unZpq+fwmzPx1G3A0173wMmkyF1lMRms5GUlGTo/pGz1B+eR33iWdQfnqU8+6No3X/Go4JNTEwMa9eudWvbs2cPzZo1c71+/fXXmT17NpMnTyY+Pv6St2WxWIz5EGSfhk3vAGDqNh6L1aO6wMWw/SMlUn94HvWJZ1F/eBYj+8Oj7mPTp08fTp06xbx588jPz2f9+vV8/vnnDBw4EIB33nmHd955h/nz519WqDHUj7MhLwOiWkJMP6OrERER8SoedbggLCyMuXPnMmXKFGbMmEF4eDiTJk2iU6dOrnvYZGdnM2TIELf3Pfjgg4wcOdKgqi9CXhYkvuGc7prgUaegREREvIHhwWbnzp1ur2NjY1mwYEGx5UwmExs3bqyossrHT+9BVjKENYSWtxpdjYiIiNfxqFNRXq0gD9bNdE5fMwYshmdKERERr6NgU1GSFsKZwxBcE9oM+fPlRURE5KIp2FQEux3WvuKc7vR/4ONvaDkiIiLeSsGmIuxYBqd2gX91uHqY0dWIiIh4LQWb8uZwwJqXndPth4N/NWPrERER8WIKNuVt33dwdDNYA6DTQ0ZXIyIi4tUUbMpb0dGatvdAUA1jaxEREfFyCjbl6fAm2P8/MFvhmtFGVyMiIuL1FGzKU9HRmtg7ILSesbWIiIhUAQo25eXEDufVUJig6zijqxEREakSFGzKS9F9a1rcCJHNDS1FRESkqlCwKQ+ph5x3Ggbo9rCxtYiIiFQhCjblYd1MsBdAox5Qp53R1YiIiFQZCjZlLeOk8yneoKM1IiIiFUzBpqwlvgEFOVC7rfOIjYiIiFQYBZuylHMGfpztnO72MJhMxtYjIiJSxSjYlKWNcyA3DWo0h+Y3Gl2NiIhIlaNgU1bys+GHWc7pruPArF0rIiJS0fTXt6xsmQ+ZJ6B6PYgdZHQ1IiIiVZKCTVnZMMf5/ZrRYPExthYREZEqymp0AV6jVmsICIe4oUZXIiIiUmUp2JSVW980ugIREZEqT6eiRERExGso2IiIiIjXULARERERr6FgIyIiIl5DwUZERES8hoKNiIiIeA0FGxEREfEaCjYiIiLiNRRsRERExGso2IiIiIjXULARERERr6FgIyIiIl5DwUZERES8hoKNiIiIeA2r0QVUNIfDAYDNZjO4Es9UtF+0fzyD+sPzqE88i/rDs5RnfxSts+jv+IWYHH+2hJfJy8sjKSnJ6DJERETkEsTGxuLr63vB+VUu2NjtdgoKCjCbzZhMJqPLERERkVJwOBzY7XasVitm84VH0lS5YCMiIiLeS4OHRURExGso2IiIiIjXULARERERr6FgIyIiIl5DwUZERES8hoKNiIiIeA0FGxEREfEaCjZCSkoKffr0ITEx0dW2detWBg0aRFxcHL1792bhwoUGVlg17Nixg/vvv58OHTrQpUsXHn30UVJSUgD1hxF++OEHBg0aRNu2benSpQvPPPMMOTk5gPrDSDabjaFDhzJx4kRXm/rDGF988QVXXnklcXFxrq8JEyYABveJQ6q0jRs3Oq677jpHTEyMY/369Q6Hw+FITU11dOjQwfH+++878vPzHevWrXPExcU5tm7danC13is7O9vRpUsXx6uvvurIzc11pKSkOIYPH+548MEH1R8GSE5OdsTGxjoWLVrksNlsjuPHjzsGDBjgePXVV9UfBnvllVccLVq0cDz22GMOh0P/Xhlp6tSpjokTJxZrN7pPdMSmCluyZAmPPPIICQkJbu0rV64kNDSUIUOGYLVa6dy5M/Hx8cyfP9+gSr3f0aNHadGiBaNGjcLX15ewsDAGDx7Mhg0b1B8GCA8PZ926ddx2222YTCZSU1PJzc0lPDxc/WGgH374gZUrV9K3b19Xm/rDOElJSVx11VXF2o3uEwWbKqxr16589dVX9O/f36199+7dxMTEuLU1bdqUHTt2VGR5VUrjxo2ZPXs2FovF1bZixQpatmyp/jBIcHAwAD169CA+Pp7IyEhuu+029YdBkpOTeeKJJ3jppZcICAhwtas/jGG32/nll1/47rvv6NWrF927d+cf//gHaWlphveJgk0VFhkZidVqLdaemZnp9g8HgL+/P1lZWRVVWpXmcDiYPn063377LU888YT6w2ArV67kf//7H2azmTFjxqg/DGC325kwYQL3338/LVq0cJun/jBGSkoKV155Jf369eOLL75gwYIFHDhwgAkTJhjeJwo2UkxAQIBrkGSRnJwcgoKCDKqo6sjIyGDMmDF8/vnnvP/++zRv3lz9YTB/f39q1qzJhAkTWL16tfrDAG+99Ra+vr4MHTq02Dz1hzFq1KjB/Pnzuf322wkICKB27dpMmDCB//3vfzgcDkP7RMFGiomJiWH37t1ubXv27KFZs2YGVVQ1HDp0iIEDB5KRkcEnn3xC8+bNAfWHEX766Seuv/568vLyXG15eXn4+PjQtGlT9UcF++yzz/jxxx+5+uqrufrqq1m2bBnLli3j6quv1ufDIDt27ODFF1/E4XC42vLy8jCbzbRq1crQPlGwkWL69OnDqVOnmDdvHvn5+axfv57PP/+cgQMHGl2a10pLS+Pee++lbdu2zJkzh/DwcNc89UfFa968OTk5Obz00kvk5eVx5MgR/vWvf3H77bfTr18/9UcF+/LLL/npp5/YuHEjGzduZMCAAQwYMICNGzfq82GQ0NBQ5s+fz+zZsykoKODo0aO88MIL3HrrrYZ/RkyOc+OWVFnNmzfnvffeo2PHjoBztPuUKVPYtWsX4eHh/N///R+33XabwVV6r3feeYepU6cSEBCAyWRym7d582b1hwH27NnDc889R1JSEiEhIcTHx7uuWlN/GKvoHjZTp04F9O+VUX788Udefvlldu3ahZ+fHzfeeCMTJkzAz8/P0D5RsBERERGvoVNRIiIi4jUUbERERMRrKNiIiIiI11CwEREREa+hYCMiIiJeQ8FGREREvIaCjYiIiHgNBRsRERHxGgo2IlKpHD16lLi4OI4ePfqnyyYmJrqeuVWSmTNnlvhgRRGpvKxGFyAicjFq167N5s2bjS5DRDyUjtiIyCU5fPgwzZs3Z+HChfTu3Zt27dpx//33c+zYsT9978SJE3nyyScZOXIkcXFxXHvttbz33nuu+RkZGUyePJkePXrQuXNnEhISOHXqlNt2Dx8+7Hr9wAMP0LZtW66//nrmzZtX7CjNnDlz6NOnD23atGHMmDFkZGS45mVlZTFx4kQ6duzIDTfcwKeffuqal5OTw7Rp0+jRowft27dn6NChbNu2zTW/efPmPPvss3Ts2JGRI0eSkZFBQkICHTt2pEuXLjzwwAPs3bv3kvaviFwaBRsRuSzfffcdn376KStWrODUqVPMmjWrVO9bvHgxQ4cOZcOGDQwfPpypU6dy/PhxAB5//HEOHjzI4sWL+frrrwkODuZvf/sb5z/azmaz8eCDDxIVFcWaNWuYM2eOWzApcuTIEZYtW8aKFSvYsmUL8+fPd837+eefueqqq1izZg2TJk1i0qRJbNy4EYCnn36aNWvW8N5777F27Vquu+467rvvPrfTYIcOHeK7775j2rRpzJ07l4yMDL7//nu+/fZbIiMjefHFFy92l4rIZVCwEZHLMnz4cKpVq0aNGjXo3bs3Bw4cKNX7io5qWK1WBg4ciM1m49ChQyQnJ7NixQqeeOIJIiIiCAoK4vHHHycpKYlffvnFbR1btmzhwIED/OMf/yAwMJA6deqQkJBQbFujR4/Gz8+PmjVr0r59ew4dOuSad8UVV3D33Xfj4+NDly5d6NevH5999hm5ubksW7aM8ePH06BBA3x9fbn33ntp3Lgxy5Ytc71/wIABBAQEUK1aNfz9/dmxYweffvopx48f57nnnuONN964tB0rIpdEY2xE5LLUqFHDNW21WosdVbmQyMhI17SPjw8AdrudI0eOAHDHHXe4LW+xWDh8+DChoaGutmPHjhEWFkZgYKCrrW7dusW2FRYW5rYtm812weVr1arFrl27SEtLIz8/v9j8unXruk6DAURFRbmmhw8fjq+vL5988gmTJ0+mXr16jB8/nr59+154R4hImVKwERGPUrNmTQCWL1/uFn727NlDvXr1OHnypKutdu3apKSkkJ2dTUBAAECprpY614kTJ9xe//bbb9SpU4caNWrg5+fHb7/9RpMmTVzzDx06RO/evV2vTSaTa3rnzp307t2b++67j/T0dD744AMSEhJYv349ISEhF1WXiFwanYoSEY9Ss2ZNevbsyZQpUzh9+jT5+fm88cYb3H777Zw5c8Zt2datW9O0aVOmTp1KdnY2x48fZ8aMGRe1vW3btrFo0SLy8/P59ttvWbVqFYMGDcJsNjNw4EBefvllDh48SF5eHu+++y579uzhxhtvLHFdCxcu5NFHHyU5OZng4GCCg4MJDAzE19f3kveHiFwcBRsR8TjTpk2jWrVq3HLLLXTq1Invv/+e2bNnux3BATCbzcyYMYMDBw7QuXNn7r33Xtq3b+86tVUa11xzDd988w0dOnTg5Zdf5tVXX+XKK68E4NFHH6Vr167cd999dOzYkeXLlzNnzhwaNWpU4roefvhhGjRowI033kjbtm1ZvHgxs2bNws/P79J3hohcFJOjtCfERUQ8TE5ODps3b6ZDhw5YLBYAVq1axVNPPcXq1asNrk5EjKAjNiJSafn4+DBu3Dg+/vhj7HY7ycnJzJ07l169ehldmogYREdsRKRMvfPOO384ziU+Pp7JkyeX2fY2btzItGnT2Lt3L35+fvTr148JEya4XSklIlWHgo2IiIh4DZ2KEhEREa+hYCMiIiJeQ8FGREREvIaCjYiIiHgNBRsRERHxGgo2IiIi4jUUbERERMRrKNiIiIiI1/h/nPuJJgCW5fcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_prev = ['PreviousYearCheckoutsSum', 'PreviousYearCheckoutsMean']\n",
    "\n",
    "neighbors = np.arange(5, 55, 5) \n",
    "train_rmse = np.empty(len(neighbors)) \n",
    "test_rmse = np.empty(len(neighbors)) \n",
    "  \n",
    "# Loop over K values \n",
    "for i, k in enumerate(neighbors): \n",
    "    print(k)\n",
    "    \n",
    "    knn = Pipeline([ ( 'scale', StandardScaler()), ('knn', KNeighborsRegressor(n_neighbors=k))])\n",
    "    #knn.fit(df_tt[new_features], df_tt['FirstYearCheckouts']) \n",
    "    knn.fit(df_tt[feature_prev], df_tt['FirstYearCheckouts']) \n",
    "      \n",
    "    # Compute training and test data accuracy \n",
    "    train_rmse[i] = root_mean_squared_error(knn.predict(df_tt[feature_prev]), df_tt['FirstYearCheckouts']) \n",
    "    test_rmse[i] = root_mean_squared_error(knn.predict(df_ho[feature_prev]), df_ho['FirstYearCheckouts']) \n",
    "  \n",
    "  \n",
    "# Generate plot \n",
    "plt.plot(neighbors, test_rmse, label = 'Testing dataset RMSE') \n",
    "plt.plot(neighbors, train_rmse, label = 'Training dataset RMSE') \n",
    "  \n",
    "plt.legend() \n",
    "plt.xlabel('n_neighbors') \n",
    "plt.ylabel('RMSE') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also try XGBoost Regressor as another possible model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.82688008798233\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and hold out set\n",
    "df_tt, df_ho = train_test_split(df_train, test_size=0.2,\\\n",
    "                                      shuffle=True, random_state=216)\n",
    "\n",
    "# Initialize model\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "# Fit model\n",
    "xgb.fit(df_tt[features], df_tt['FirstYearCheckouts'])\n",
    "\n",
    "# Make predictions\n",
    "pred = xgb.predict(df_ho[features])\n",
    "\n",
    "rmse = root_mean_squared_error(pred, df_ho['FirstYearCheckouts'])\n",
    "\n",
    "print('RMSE for XGBoost Regressor:', rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, by a small magnitude, XGBoost Regressor seems to perform the best of the models we have considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also try a small NN to see if it can capture any additional structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for NN with 1 hidden layers each with 100 neurons: 106.00505114384612\n"
     ]
    }
   ],
   "source": [
    "nn = MLPRegressor(\n",
    "    activation='relu',\n",
    "    hidden_layer_sizes=(100,),\n",
    "    alpha=0.001,\n",
    "    random_state=20,\n",
    "    early_stopping=False\n",
    ")\n",
    "\n",
    "nn.fit(df_tt[features], df_tt['FirstYearCheckouts'])\n",
    "\n",
    "pred = nn.predict(df_ho[features])\n",
    "\n",
    "rmse = root_mean_squared_error(pred, df_ho['FirstYearCheckouts'])\n",
    "\n",
    "print('RMSE for NN with 1 hidden layers each with 100 neurons:', rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that this does not improve our estimate compared with RandomForest and XgBoost Regressor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting log(FirstYearCheckouts):\n",
    "\n",
    "We could also look at predicting the order of magnitude of the first year checkouts, and consider instead predicting on the log of the FirstYearCheckouts with all the same models as above besides KNN as this did not provide much improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['logFYC'] = np.log10(df_train['FirstYearCheckouts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make KFold object to be used on training dataset\n",
    "kfold = KFold(n_splits = 5,\n",
    "              shuffle = True,\n",
    "              random_state = 216)\n",
    "\n",
    "\n",
    "rmses = np.zeros(5)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(df_train, df_train['logFYC'])):\n",
    "    ## get the kfold training and holdout data\n",
    "    X_tt = df_train.iloc[train_index]\n",
    "    X_ho = df_train.iloc[test_index]\n",
    "\n",
    "    #Get average of the first year checkouts for our train set \n",
    "    baseline_pred = np.ones(len(X_ho)) * X_tt['logFYC'].mean() \n",
    "\n",
    "    ## Record the rmses\n",
    "    rmses[i] = root_mean_squared_error(X_ho['logFYC'], baseline_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.54403594 0.54392708 0.54377702 0.5427933  0.54642483]\n",
      "0.5441916335734021\n"
     ]
    }
   ],
   "source": [
    "print(rmses)\n",
    "print(rmses.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4673685045253166"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**0.54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the RMSE is in log-space it behaves like a multipicative factor.  Thus, the model's predictions are, on average, about 3.47 times away from the actual values in the original space, providing a measure of multiplicative error rather than absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of log first year checkouts for the whole dataset is 1.2983440300851525\n",
      "The minimum and maximum log first year checkouts in our dataset are 0.0 and 3.983761560286165 respectively\n"
     ]
    }
   ],
   "source": [
    "print('The average number of log first year checkouts for the whole dataset is', df_train['logFYC'].mean())\n",
    "print('The minimum and maximum log first year checkouts in our dataset are', df_train['logFYC'].min(), \\\n",
    "      'and', df_train['logFYC'].max(), 'respectively')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to store rmse\n",
    "rmse = np.zeros(5)\n",
    "\n",
    "# Initialize LinearRegression Model\n",
    "lr = LinearRegression()\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(df_train, df_train['logFYC'])):\n",
    "\n",
    "    ## get the kfold training and holdout data\n",
    "    X_tt = df_train.iloc[train_index]\n",
    "    X_ho = df_train.iloc[test_index]\n",
    "\n",
    "    ## Fit model\n",
    "    lr.fit(X_tt[features], X_tt['logFYC'])\n",
    "\n",
    "    ## Generate predictions on the holdout set\n",
    "    lr_preds = lr.predict(X_ho[features])\n",
    "\n",
    "    ## Record the rmses\n",
    "    rmses[i] = root_mean_squared_error(X_ho['logFYC'], lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49952901 0.49982952 0.49544793 0.49652082 0.50059854]\n",
      "0.49838516534916044\n"
     ]
    }
   ],
   "source": [
    "print(rmses)\n",
    "print(rmses.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Creator', 'UsageClass', 'CheckoutMonth', 'CheckoutYear',\n",
       "       'PreviousYearCheckoutsSum', 'PreviousYearCheckoutsMean',\n",
       "       'PreviousYearPublished', 'PreviousYearPublishedMean',\n",
       "       'FirstYearCheckouts', 'other', 'juvenile', 'mystery', 'history',\n",
       "       'nonfiction', 'fiction', 'romance', 'fantasy/sci-fi', 'biography',\n",
       "       'young adult', 'horror/thriller', 'BOOK', 'EBOOK', 'AUDIOBOOK',\n",
       "       'SOUNDDISC', 'OTHER', 'VIDEODISC', 'other publisher',\n",
       "       'simon & schuster', 'harpercollins', 'penguin random house', 'hachette',\n",
       "       'blackstone', 'harlequin', 'macmillan', 'scholastic',\n",
       "       'lightning source', 'recorded books', 'brilliance', 'logFYC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.40904772203186623\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df_tt, df_ho = train_test_split(df_train, test_size=0.2,\\\n",
    "                                      shuffle=True, random_state=216)\n",
    "\n",
    "# Create a Random Forest Regressor model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(df_tt[features], df_tt['logFYC'])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(df_ho[features])\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = root_mean_squared_error(df_ho['logFYC'], y_pred)\n",
    "print(\"Root Mean Squared Error:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.51188643150958"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG Boost Regressor Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for XGBoost Regressor: 0.4020223885124308\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and hold out set\n",
    "df_tt, df_ho = train_test_split(df_train, test_size=0.2,\\\n",
    "                                      shuffle=True, random_state=216)\n",
    "\n",
    "# Initialize model\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "# Fit model\n",
    "xgb.fit(df_tt[features], df_tt['logFYC'])\n",
    "\n",
    "# Make predictions\n",
    "pred = xgb.predict(df_ho[features])\n",
    "\n",
    "rmse = root_mean_squared_error(pred, df_ho['logFYC'])\n",
    "\n",
    "print('RMSE for XGBoost Regressor:', rmse)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
